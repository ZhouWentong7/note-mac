蕴含了单词含义的表示方法：

- 基于同义词词典的方法 本章  
- 基于计数的方法 本章  
- 基于推理的方法（word2vec） 下一章（基于神经网络的推理方法）

## 同义词词典

同义词（thesaurus）将相同或含义近似的词语归为一组。
据各单词的含义，基于上位-下位关系形成的图。

![[fig2-2.png]]

### WordNet
是一个大型的词汇数据库，可以看作是**词典和同义词词典的结合与扩展**。但它最独特的地方在于，它的组织方式不是按字母顺序排列，而是根据词语的**语义关系**来构建的。

可以通过nltk模块使用。

**缺点**
- 难以顺应时代变化的词义变化
- 人力成本高
- 单词微妙差别难以识别
### 基于计数的方法

**语料库（corpus）**：结构化的为NLP和应用而专门收集的文本数据。

####  常见语料库

Wikipedia

Google News

#### 语料预处理

1. 统一成小写
2. 处理标点符号： 替换`.` 为` .`
3. 通过空格切分语料，一个单词一格
4. 建立三个字典：`corpus` 是单词 ID列表，`word_to_id` 是单词到单词ID的字典，`id_to_word` 是单词ID到单词的字典

#### 分布式假设
**分布式假设（distributional hypothesis）** ：“某个单词的含义由它周围的单词形成”

- 单词含义由在文本附近（上下文、语境）的单词决定
- 这个范围大小称之为 窗口

#### 共现矩阵

向量表示单词的方法
- 计数
- 基于分布式假设使用

![[fig2-7.png]]

#### 向量间相似度

##### 余弦相似度（cosine similarity）
$$
\text{cosine\_similarity} = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \, \|\mathbf{B}\|}
$$

分子是向量内积，分母是各个向量的范数。

这里计算的是L2范数（即向量各个元素的平方和的平方根）。

> 记得加上微小值防止“0”作为除数发生。

```python
def cos_similarity(x, y, eps=1e-8):
	'''计算余弦相似度
	:param x: 向量
	:param y: 向量
	:param eps: 用于防止“除数为0”的微小值
	:return:
	'''

	nx = x / (np.sqrt(np.sum(x ** 2)) + eps)
	
	ny = y / (np.sqrt(np.sum(y ** 2)) + eps)
	
	return np.dot(nx, ny)
```

#### 相似单词的排序
❶ 取出查询词的单词向量。  
❷ 分别求得查询词的单词向量和其他所有单词向量的余弦相似度。  
❸ 基于余弦相似度的结果，按降序显示它们的值 （argsort()方法）

### 基于计数方法的改进

#### 点互信息（Pointwise Mutual Information， PMI）

$$

PMI(x,y) = log_2 \frac{P(x,y)}{P(x)P(y)}

$$
P(x)表示x发生的概率，P(y)表示y发生的概率，P(x,y)表示x 和y同时发生的概率。**PMI的值越高，表明相关性越强。**
- P(x)就是指单词x在语料库中出现的概率。

但是，当单词出现概率为0的时候，$PMI=-\infty$

正点互信息：

$$ PPMI =  max(0,PMI(x,y))$$
但是，这个PPMI矩阵还是存在一个很大的问题，那就是随着语料库 的词汇量增加，各个单词向量的维数也会增加。

经过观察，PPMI矩阵中大部分值为0，可以通过**降维**的方式压缩存储空间。

#### 降维

尽量保留“**重要信息**”的基础上减少**向量维度**。
![[fig2-8.png]]
找到一根新的轴，通过计算在这个新的轴上的投影以表示数据。

##### 奇异值分解 Singular Value  Decomposition，SVD
#奇异值分解 #SVD

$$
X = USV^T
$$
  
U和V是列向量彼此正交的正交矩阵，S是除了对角线元素以外其余元素均为0的对角矩阵。
![[fig2-9.png]]

- U作为“单词空间”
- 奇异值在S上降序排列
- 矩阵S的奇异值小，对应的基轴的重要性低，因此，可以通过去除矩阵U中的多余的列向量来近似原始矩阵

```python
# SVD

U, S, V = np.linalg.svd(W)

np.set_printoptions(precision=3) # 有效位数为3位
print(C[0])
print(W[0])
print(U[0])

# plot

for word, word_id in word_to_id.items()
plt.annotate(word, (U[word_id, 0], U[word_id, 1]))
plt.scatter(U[:,0], U[:,1], alpha=0.5)
plt.show()
```

#### PTB数据集

##### 基于PTB数据集的评价

```cmd
[query] you
 i: 0.6669368743896484
 we: 0.6337743997573853
 anybody: 0.5424059629440308
 do: 0.528918981552124
 someone: 0.528694212436676

[query] year
 month: 0.6644545793533325
 quarter: 0.6414061188697815
 earlier: 0.5933421850204468
 last: 0.5880612134933472
 next: 0.5731372833251953

[query] car
 auto: 0.5897628664970398
 luxury: 0.5661390423774719
 domestic: 0.539510190486908
 vehicle: 0.5305922031402588
 cars: 0.5186667442321777

[query] toyota
 motor: 0.7153434753417969
 nissan: 0.6580232977867126
 motors: 0.6493822932243347
 mazda: 0.6074504256248474
 honda: 0.6056519150733948
```

