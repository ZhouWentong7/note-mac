---
annotation-target: file:/Users/zhou-wentong/Documents/books/深度学习进阶-自然语言处理.pdf
---


>%%
>```annotation-json
>{"created":"2025-08-13T00:52:14.404Z","text":"Githu仓库\n","updated":"2025-08-13T00:52:14.404Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":17756,"end":17817},{"type":"TextQuoteSelector","exact":"https://github.com/oreilly-japan/deep-learning-from-scratch-2","prefix":"ly.co.jp/books/9784873118369（日语）","suffix":"  前言xx关于O’Reilly的其他信息，可以访问下面的O’R"}]}]}
>```
>%%
>*%%PREFIX%%ly.co.jp/books/9784873118369（日语）%%HIGHLIGHT%% ==https://github.com/oreilly-japan/deep-learning-from-scratch-2== %%POSTFIX%%前言xx关于O’Reilly的其他信息，可以访问下面的O’R*
>%%LINK%%[[#^fposwz6dju9|show annotation]]
>%%COMMENT%%
>Githu仓库
>
>%%TAGS%%
>#github, #code
^fposwz6dju9


>%%
>```annotation-json
>{"created":"2025-08-13T01:08:51.425Z","text":"多类别分类：有很多类别，但只属于其中某一类\n多标签：有很多类别，可能被分配多个标签（使用sigmoid）","updated":"2025-08-13T01:08:51.425Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":28019,"end":28068},{"type":"TextQuoteSelector","exact":"进行多类别分类的神经网络通常使用交叉熵误差（cross entropy error）作为损失函数。","prefix":"计算神经网络的损失要使用损失函数（loss function）。","suffix":"此时，交叉熵误差由神经网络输出的各类别的概率和监督标签求得。现在"}]}]}
>```
>%%
>*%%PREFIX%%计算神经网络的损失要使用损失函数（loss function）。%%HIGHLIGHT%% ==进行多类别分类的神经网络通常使用交叉熵误差（cross entropy error）作为损失函数。== %%POSTFIX%%此时，交叉熵误差由神经网络输出的各类别的概率和监督标签求得。现在*
>%%LINK%%[[#^ud1pfn0cuxq|show annotation]]
>%%COMMENT%%
>多类别分类：有很多类别，但只属于其中某一类
>多标签：有很多类别，可能被分配多个标签（使用sigmoid）
>%%TAGS%%
>#ce-loss
^ud1pfn0cuxq


>%%
>```annotation-json
>{"created":"2025-08-13T01:13:20.344Z","text":"交叉熵误差","updated":"2025-08-13T01:13:20.344Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":28743,"end":28760},{"type":"TextQuoteSelector","exact":"=−∑ktklogyk (1.7)","prefix":"，这个概率会被输入交叉熵误差。此时，交叉熵误差可由下式表示： L","suffix":"  第1章 神经网络的复习20这里，tk是对应于第k个类别的监督"}]}]}
>```
>%%
>*%%PREFIX%%，这个概率会被输入交叉熵误差。此时，交叉熵误差可由下式表示： L%%HIGHLIGHT%% ===−∑ktklogyk (1.7)== %%POSTFIX%%第1章 神经网络的复习20这里，tk是对应于第k个类别的监督*
>%%LINK%%[[#^xf713toupbi|show annotation]]
>%%COMMENT%%
>交叉熵误差
>%%TAGS%%
>
^xf713toupbi


>%%
>```annotation-json
>{"created":"2025-08-13T01:18:39.894Z","text":"有多个自变量时，分别计算因变量关于每个自变量的偏导，罗列到一起，就是梯度（gradient）。\n严格来说，和数学领域中的梯度略有区别。","updated":"2025-08-13T01:18:39.894Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":29964,"end":29987},{"type":"TextQuoteSelector","exact":"将关于向量各个元素的导数罗列到一起，就得到了梯","prefix":"∂L∂x1,∂L∂x2,...,∂L∂xn) (1.9)像这样，","suffix":"度（gradient）。另外，矩阵也可以像向量一样求梯度。假设W"}]}]}
>```
>%%
>*%%PREFIX%%∂L∂x1,∂L∂x2,...,∂L∂xn) (1.9)像这样，%%HIGHLIGHT%% ==将关于向量各个元素的导数罗列到一起，就得到了梯== %%POSTFIX%%度（gradient）。另外，矩阵也可以像向量一样求梯度。假设W*
>%%LINK%%[[#^gmqohl5bqpq|show annotation]]
>%%COMMENT%%
>有多个自变量时，分别计算因变量关于每个自变量的偏导，罗列到一起，就是梯度（gradient）。
>严格来说，和数学领域中的梯度略有区别。
>%%TAGS%%
>#梯度, #gradient
^gmqohl5bqpq


>%%
>```annotation-json
>{"created":"2025-08-13T01:18:13.974Z","text":"导数含义","updated":"2025-08-13T01:18:13.974Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":29646,"end":29683},{"type":"TextQuoteSelector","exact":"就是x的微小（严格地讲，“微小”为无限小）变化会导致y发生多大程度的变化。","prefix":"的导数记为 dydx。这个 dydx的意思是变化程度，具体来说，","suffix":"比如函数y=x2，其导数可以解析性地求出，即 dydx=2x。这"}]}]}
>```
>%%
>*%%PREFIX%%的导数记为 dydx。这个 dydx的意思是变化程度，具体来说，%%HIGHLIGHT%% ==就是x的微小（严格地讲，“微小”为无限小）变化会导致y发生多大程度的变化。== %%POSTFIX%%比如函数y=x2，其导数可以解析性地求出，即 dydx=2x。这*
>%%LINK%%[[#^4dxioxzs5we|show annotation]]
>%%COMMENT%%
>导数含义
>%%TAGS%%
>
^4dxioxzs5we


>%%
>```annotation-json
>{"created":"2025-08-13T01:20:10.692Z","text":"和数学中的梯度的区别\n","updated":"2025-08-13T01:20:10.692Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":30330,"end":30379},{"type":"TextQuoteSelector","exact":"数学中的梯度仅限于关于向量的导数。而在深度学习领域，一般也会定义关于矩阵和张量的导数，称为“梯度”","prefix":"现。严格地说，本书使用的“梯度”一词与数学中的“梯度”是不同的。","suffix":"。1.3.3  链式法则学习阶段的神经网络在给定学习数据后会输出"}]}]}
>```
>%%
>*%%PREFIX%%现。严格地说，本书使用的“梯度”一词与数学中的“梯度”是不同的。%%HIGHLIGHT%% ==数学中的梯度仅限于关于向量的导数。而在深度学习领域，一般也会定义关于矩阵和张量的导数，称为“梯度”== %%POSTFIX%%。1.3.3  链式法则学习阶段的神经网络在给定学习数据后会输出*
>%%LINK%%[[#^cot1wtvxc65|show annotation]]
>%%COMMENT%%
>和数学中的梯度的区别
>
>%%TAGS%%
>
^cot1wtvxc65


>%%
>```annotation-json
>{"created":"2025-08-13T07:40:49.862Z","text":"找到损失增加最多的方向，顺着这个方向反向更新就下降最快\n","updated":"2025-08-13T07:40:49.862Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":39329,"end":39383},{"type":"TextQuoteSelector","exact":"这个梯度指向当前的权重参数所处位置中损失增加最多的方向。因此，通过将参数向该梯度的反方向更新，可以降低损失。","prefix":"选择mini-batch数据，根据误差反向传播法获得权重的梯度。","suffix":"这就是梯度下降法（gradient descent）。之后，根据"}]}]}
>```
>%%
>*%%PREFIX%%选择mini-batch数据，根据误差反向传播法获得权重的梯度。%%HIGHLIGHT%% ==这个梯度指向当前的权重参数所处位置中损失增加最多的方向。因此，通过将参数向该梯度的反方向更新，可以降低损失。== %%POSTFIX%%这就是梯度下降法（gradient descent）。之后，根据*
>%%LINK%%[[#^8fqldro6ye|show annotation]]
>%%COMMENT%%
>找到损失增加最多的方向，顺着这个方向反向更新就下降最快
>
>%%TAGS%%
>
^8fqldro6ye


>%%
>```annotation-json
>{"created":"2025-08-13T01:12:32.484Z","text":"所有类别的输出和为1，可以解释为概率。\n","updated":"2025-08-13T01:12:32.484Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":28639,"end":28666},{"type":"TextQuoteSelector","exact":"Softmax函数输出的各个元素是0.0～1.0的实数","prefix":"数的分子是得分sk的指数函数，分母是所有输入信号的指数函数的和。","suffix":"。另外，如果将这些元素全部加起来，则和为1。因此，Softmax"}]}]}
>```
>%%
>*%%PREFIX%%数的分子是得分sk的指数函数，分母是所有输入信号的指数函数的和。%%HIGHLIGHT%% ==Softmax函数输出的各个元素是0.0～1.0的实数== %%POSTFIX%%。另外，如果将这些元素全部加起来，则和为1。因此，Softmax*
>%%LINK%%[[#^2kenz90y9vo|show annotation]]
>%%COMMENT%%
>所有类别的输出和为1，可以解释为概率。
>
>%%TAGS%%
>
^2kenz90y9vo


>%%
>```annotation-json
>{"created":"2025-08-13T08:24:50.520Z","updated":"2025-08-13T08:24:50.520Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":52298,"end":52342},{"type":"TextQuoteSelector","exact":"在同义词词典中，具有相同含义的单词（同义词）或含义类似的单词（近义词）被归类到同一个组中","prefix":"常规词典，而是一种被称为同义词词典（thesaurus）的词典。","suffix":"。比如，使用同义词词典，我们可以知道car的同义词有automo"}]}]}
>```
>%%
>*%%PREFIX%%常规词典，而是一种被称为同义词词典（thesaurus）的词典。%%HIGHLIGHT%% ==在同义词词典中，具有相同含义的单词（同义词）或含义类似的单词（近义词）被归类到同一个组中== %%POSTFIX%%。比如，使用同义词词典，我们可以知道car的同义词有automo*
>%%LINK%%[[#^hbevd1548y7|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#同义词词典
^hbevd1548y7


>%%
>```annotation-json
>{"created":"2025-08-13T08:28:07.263Z","updated":"2025-08-13T08:28:07.263Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":53099,"end":53149},{"type":"TextQuoteSelector","exact":"使用WordNet，可以获得单词的近义词，或者利用单词网络。使用单词网络，可以计算单词之间的相似度。","prefix":"同义词词典，迄今已用于许多研究，并活跃于各种自然语言处理应用中。","suffix":"这里，我们不对WordNet进行详细说明，对WordNet的Py"}]}]}
>```
>%%
>*%%PREFIX%%同义词词典，迄今已用于许多研究，并活跃于各种自然语言处理应用中。%%HIGHLIGHT%% ==使用WordNet，可以获得单词的近义词，或者利用单词网络。使用单词网络，可以计算单词之间的相似度。== %%POSTFIX%%这里，我们不对WordNet进行详细说明，对WordNet的Py*
>%%LINK%%[[#^j3qnwbwh5og|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#WordNet
^j3qnwbwh5og


>%%
>```annotation-json
>{"created":"2025-08-13T08:32:27.399Z","text":"是基于计数的方法，为NLP研究和应而专门手机的数据。","updated":"2025-08-13T08:32:27.399Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":54307,"end":54318},{"type":"TextQuoteSelector","exact":"语料库（corpus）","prefix":" 632.3 基于计数的方法从介绍基于计数的方法开始，我们将使用","suffix":"。简而言之，语料库就是大量的文本数据。不过，语料库并不是胡乱收集"}]}]}
>```
>%%
>*%%PREFIX%%632.3 基于计数的方法从介绍基于计数的方法开始，我们将使用%%HIGHLIGHT%% ==语料库（corpus）== %%POSTFIX%%。简而言之，语料库就是大量的文本数据。不过，语料库并不是胡乱收集*
>%%LINK%%[[#^asdmjt6lqmu|show annotation]]
>%%COMMENT%%
>是基于计数的方法，为NLP研究和应而专门手机的数据。
>%%TAGS%%
>#语料库, #corpus
^asdmjt6lqmu


>%%
>```annotation-json
>{"created":"2025-08-13T08:36:48.414Z","text":"语料预处理代码（英文文本）","updated":"2025-08-13T08:36:48.414Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":56745,"end":56753},{"type":"TextQuoteSelector","exact":"语料库的准备工作","prefix":"  第2章 自然语言和单词的分布式表示66至此，我们就完成了利用","suffix":"。现在，我们将上述一系列处理实现为preprocess() 函数"}]}]}
>```
>%%
>*%%PREFIX%%第2章 自然语言和单词的分布式表示66至此，我们就完成了利用%%HIGHLIGHT%% ==语料库的准备工作== %%POSTFIX%%。现在，我们将上述一系列处理实现为preprocess() 函数*
>%%LINK%%[[#^3uevkyzj5pl|show annotation]]
>%%COMMENT%%
>语料预处理代码（英文文本）
>%%TAGS%%
>
^3uevkyzj5pl


>%%
>```annotation-json
>{"created":"2025-08-13T08:39:59.030Z","text":"有趣","updated":"2025-08-13T08:39:59.030Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":58250,"end":58309},{"type":"TextQuoteSelector","exact":"这个想法就是“某个单词的含义由它周围的单词形成”，称为分布式假设（distributional hypothesis）","prefix":"看一下这些研究，就会发现几乎所有的重要方法都基于一个简单的想法，","suffix":"。许多用向量表示单词的近期研究也基于该假设。分布式假设所表达的理"}]}]}
>```
>%%
>*%%PREFIX%%看一下这些研究，就会发现几乎所有的重要方法都基于一个简单的想法，%%HIGHLIGHT%% ==这个想法就是“某个单词的含义由它周围的单词形成”，称为分布式假设（distributional hypothesis）== %%POSTFIX%%。许多用向量表示单词的近期研究也基于该假设。分布式假设所表达的理*
>%%LINK%%[[#^fynp0i8zg1|show annotation]]
>%%COMMENT%%
>有趣
>%%TAGS%%
>#分布式假设, #distributional hypothesis
^fynp0i8zg1


>%%
>```annotation-json
>{"created":"2025-08-13T08:48:12.441Z","updated":"2025-08-13T08:48:12.441Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":61817,"end":61841},{"type":"TextQuoteSelector","exact":"余弦相似度（cosine similarity）","prefix":"虽然除此之外还有很多方法，但是在测量单词的向量表示的相似度方面，","suffix":"是很常用的。设有x=(x1,x2,x3,···,xn)和y=(y"}]}]}
>```
>%%
>*%%PREFIX%%虽然除此之外还有很多方法，但是在测量单词的向量表示的相似度方面，%%HIGHLIGHT%% ==余弦相似度（cosine similarity）== %%POSTFIX%%是很常用的。设有x=(x1,x2,x3,···,xn)和y=(y*
>%%LINK%%[[#^6ntbkl9jwmn|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#余弦相似度
^6ntbkl9jwmn


>%%
>```annotation-json
>{"created":"2025-08-13T08:39:14.310Z","text":"单词的分布表示，就是处理成可以计算机处理的向量。","updated":"2025-08-13T08:39:14.310Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":58026,"end":58048},{"type":"TextQuoteSelector","exact":"单词的分布式表示将单词表示为固定长度的向量。","prefix":"确把握单词含义的向量表示。在自然语言处理领域，这称为分布式表示。","suffix":"这种向量的特征在于它是用密集向量表示的。密集向量的意思是，向量的"}]}]}
>```
>%%
>*%%PREFIX%%确把握单词含义的向量表示。在自然语言处理领域，这称为分布式表示。%%HIGHLIGHT%% ==单词的分布式表示将单词表示为固定长度的向量。== %%POSTFIX%%这种向量的特征在于它是用密集向量表示的。密集向量的意思是，向量的*
>%%LINK%%[[#^ratkqqrhmnj|show annotation]]
>%%COMMENT%%
>单词的分布表示，就是处理成可以计算机处理的向量。
>%%TAGS%%
>#单词分布表示
^ratkqqrhmnj


>%%
>```annotation-json
>{"created":"2025-08-13T08:56:08.044Z","updated":"2025-08-13T08:56:08.044Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":66432,"end":66434},{"type":"TextQuoteSelector","exact":"(2","prefix":"明）： PMI(x,y)=log2P(x,y)P(x)P(y) ","suffix":".2)  第2章 自然语言和单词的分布式表示78其中，P(x)表"}]}]}
>```
>%%
>*%%PREFIX%%明）： PMI(x,y)=log2P(x,y)P(x)P(y)%%HIGHLIGHT%% ==(2== %%POSTFIX%%.2)  第2章 自然语言和单词的分布式表示78其中，P(x)表*
>%%LINK%%[[#^g0p35nd1isb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#点互信息（PMI), #PMI)
^g0p35nd1isb


>%%
>```annotation-json
>{"created":"2025-08-13T09:04:09.286Z","text":"奇异值分解的理解","updated":"2025-08-13T09:04:09.286Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":70479,"end":70499},{"type":"TextQuoteSelector","exact":"S是对角矩阵，奇异值在对角线上降序排列。","prefix":"了一些空间中的基轴（基向量），我们可以将矩阵U作为“单词空间”。","suffix":"简单地说，我们可以将奇异值视为“对应的基轴”的重要性。这样一来，"}]}]}
>```
>%%
>*%%PREFIX%%了一些空间中的基轴（基向量），我们可以将矩阵U作为“单词空间”。%%HIGHLIGHT%% ==S是对角矩阵，奇异值在对角线上降序排列。== %%POSTFIX%%简单地说，我们可以将奇异值视为“对应的基轴”的重要性。这样一来，*
>%%LINK%%[[#^mkzqp8ezprp|show annotation]]
>%%COMMENT%%
>奇异值分解的理解
>%%TAGS%%
>
^mkzqp8ezprp


>%%
>```annotation-json
>{"created":"2025-08-13T09:04:52.476Z","updated":"2025-08-13T09:04:52.476Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":70605,"end":70652},{"type":"TextQuoteSelector","exact":"矩阵S的奇异值小，对应的基轴的重要性低，因此，可以通过去除矩阵U中的多余的列向量来近似原始矩阵","prefix":"VT䃺ࢂID 䃺ࢂIDࢂ䃺ा䛼䭺㐡ऻ⮱ࢂ䃺ा䛼如图2-10所示，","suffix":"。用我们正在处理的“单词的PPMI矩阵”来说明的话，矩阵X的各行"}]}]}
>```
>%%
>*%%PREFIX%%VT䃺ࢂID 䃺ࢂIDࢂ䃺ा䛼䭺㐡ऻ⮱ࢂ䃺ा䛼如图2-10所示，%%HIGHLIGHT%% ==矩阵S的奇异值小，对应的基轴的重要性低，因此，可以通过去除矩阵U中的多余的列向量来近似原始矩阵== %%POSTFIX%%。用我们正在处理的“单词的PPMI矩阵”来说明的话，矩阵X的各行*
>%%LINK%%[[#^4xgr7s7ra1d|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^4xgr7s7ra1d


>%%
>```annotation-json
>{"created":"2025-08-13T09:01:27.847Z","text":"降维的方法：奇异值分解","updated":"2025-08-13T09:01:27.847Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":70189,"end":70234},{"type":"TextQuoteSelector","exact":"这里我们使用奇异值分解（Singular Value Decomposition，SVD）","prefix":"。这个密集矩阵就是我们想要的单词的分布式表示。降维的方法有很多，","suffix":"。 SVD将任意矩阵分解为3个矩阵的乘积，如下式所示： X=US"}]}]}
>```
>%%
>*%%PREFIX%%。这个密集矩阵就是我们想要的单词的分布式表示。降维的方法有很多，%%HIGHLIGHT%% ==这里我们使用奇异值分解（Singular Value Decomposition，SVD）== %%POSTFIX%%。 SVD将任意矩阵分解为3个矩阵的乘积，如下式所示： X=US*
>%%LINK%%[[#^z675mifksxg|show annotation]]
>%%COMMENT%%
>降维的方法：奇异值分解
>%%TAGS%%
>#奇异值分解, #Singular Value Decomposition
^z675mifksxg


>%%
>```annotation-json
>{"created":"2025-08-13T09:08:10.425Z","updated":"2025-08-13T09:08:10.425Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":72597,"end":72624},{"type":"TextQuoteSelector","exact":"Penn Treebank语料库（以下简称为PTB）。","prefix":"据作为语料库。这里，我们将使用一个大小合适的“真正的”语料库——","suffix":"PTB语料库经常被用作评价提案方法的基准。本书中我们将使用PTB"}]}]}
>```
>%%
>*%%PREFIX%%据作为语料库。这里，我们将使用一个大小合适的“真正的”语料库——%%HIGHLIGHT%% ==Penn Treebank语料库（以下简称为PTB）。== %%POSTFIX%%PTB语料库经常被用作评价提案方法的基准。本书中我们将使用PTB*
>%%LINK%%[[#^yc9hwi6vy2p|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^yc9hwi6vy2p


>%%
>```annotation-json
>{"created":"2025-08-13T09:08:15.027Z","updated":"2025-08-13T09:08:15.027Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":72666,"end":72720},{"type":"TextQuoteSelector","exact":"我们使用的PTB语料库在word2vec的发明者托马斯·米科洛夫（Tomas Mikolov）的网页上有提供","prefix":"作评价提案方法的基准。本书中我们将使用PTB语料库进行各种实验。","suffix":"。这个PTB语料库是以文本文件的形式提供的，与原始的PTB的文章"}]}]}
>```
>%%
>*%%PREFIX%%作评价提案方法的基准。本书中我们将使用PTB语料库进行各种实验。%%HIGHLIGHT%% ==我们使用的PTB语料库在word2vec的发明者托马斯·米科洛夫（Tomas Mikolov）的网页上有提供== %%POSTFIX%%。这个PTB语料库是以文本文件的形式提供的，与原始的PTB的文章*
>%%LINK%%[[#^vxdkimzfx|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^vxdkimzfx


>%%
>```annotation-json
>{"created":"2025-08-13T09:08:31.978Z","updated":"2025-08-13T09:08:31.978Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":72764,"end":72810},{"type":"TextQuoteSelector","exact":"括将稀有单词替换成特殊字符<unk>（unk是unknown的简称），将具体的数字替换成“N","prefix":"本文件的形式提供的，与原始的PTB的文章相比，多了若干预处理，包","suffix":"”等。下面，我们将经过这些预处理之后的文本数据作为PTB语料库使"}]}]}
>```
>%%
>*%%PREFIX%%本文件的形式提供的，与原始的PTB的文章相比，多了若干预处理，包%%HIGHLIGHT%% ==括将稀有单词替换成特殊字符<unk>（unk是unknown的简称），将具体的数字替换成“N== %%POSTFIX%%”等。下面，我们将经过这些预处理之后的文本数据作为PTB语料库使*
>%%LINK%%[[#^11xk7fl1vdmm|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^11xk7fl1vdmm


>%%
>```annotation-json
>{"created":"2025-08-13T08:59:43.452Z","updated":"2025-08-13T08:59:43.452Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":69764,"end":69792},{"type":"TextQuoteSelector","exact":"降维（dimensionality reduction）","prefix":"。对于这些问题，一个常见的方法是向量降维。2.4.2  降维所谓","suffix":"，顾名思义，就是减少向量维度。但是，并不是简单地减少，而是在尽量"}]}]}
>```
>%%
>*%%PREFIX%%。对于这些问题，一个常见的方法是向量降维。2.4.2  降维所谓%%HIGHLIGHT%% ==降维（dimensionality reduction）== %%POSTFIX%%，顾名思义，就是减少向量维度。但是，并不是简单地减少，而是在尽量*
>%%LINK%%[[#^vxz3a7ajgvf|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#降维基础
^vxz3a7ajgvf


>%%
>```annotation-json
>{"created":"2025-08-13T13:09:12.791Z","updated":"2025-08-13T13:09:12.791Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":78628,"end":78688},{"type":"TextQuoteSelector","exact":"于推理的方法的主要操作是“推理”。如图3-2所示，当给出周围的单词（上下文）时，预测“？”处会出现什么单词，这就是推理。","prefix":"们会在3.5.3节再次讨论。3.1.2  基于推理的方法的概要基","suffix":"you and i say hello.goodbye?图3-2"}]}]}
>```
>%%
>*%%PREFIX%%们会在3.5.3节再次讨论。3.1.2  基于推理的方法的概要基%%HIGHLIGHT%% ==于推理的方法的主要操作是“推理”。如图3-2所示，当给出周围的单词（上下文）时，预测“？”处会出现什么单词，这就是推理。== %%POSTFIX%%you and i say hello.goodbye?图3-2*
>%%LINK%%[[#^ey1vpefpoo8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ey1vpefpoo8


>%%
>```annotation-json
>{"created":"2025-08-13T15:58:33.587Z","updated":"2025-08-13T15:58:33.587Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":79768,"end":79805},{"type":"TextQuoteSelector","exact":"只要将单词转化为固定长度的向量，神经网络的输入层的神经元个数就可以固定下来","prefix":"等的向量，并将单词ID对应的元素设为1，其他元素设为0。像这样，","suffix":"（图3-5）。youyou(1, 0, 0, 0, 0, 0, "}]}]}
>```
>%%
>*%%PREFIX%%等的向量，并将单词ID对应的元素设为1，其他元素设为0。像这样，%%HIGHLIGHT%% ==只要将单词转化为固定长度的向量，神经网络的输入层的神经元个数就可以固定下来== %%POSTFIX%%（图3-5）。youyou(1, 0, 0, 0, 0, 0,*
>%%LINK%%[[#^uvhu2eqlm4|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^uvhu2eqlm4


>%%
>```annotation-json
>{"created":"2025-08-13T16:01:01.896Z","updated":"2025-08-13T16:01:01.896Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":81679,"end":81708},{"type":"TextQuoteSelector","exact":"continuous bag-of-words（CBOW）","prefix":"-3所示的模型中。这里，我们使用由原版word2vec提出的名为","suffix":"的模型作为神经网络。word2vec一词最初用来指程序或者工具，"}]}]}
>```
>%%
>*%%PREFIX%%-3所示的模型中。这里，我们使用由原版word2vec提出的名为%%HIGHLIGHT%% ==continuous bag-of-words（CBOW）== %%POSTFIX%%的模型作为神经网络。word2vec一词最初用来指程序或者工具，*
>%%LINK%%[[#^5l4xd2wv8ys|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^5l4xd2wv8ys


>%%
>```annotation-json
>{"created":"2025-08-15T04:33:33.764Z","updated":"2025-08-15T04:33:33.764Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":81962,"end":81976},{"type":"TextQuoteSelector","exact":"CBOW模型的输入是上下文。","prefix":"型，使其能尽可能地进行正确的预测，我们可以获得单词的分布式表示。","suffix":"这个上下文用['you', 'goodbye'] 这样的单词列表"}]}]}
>```
>%%
>*%%PREFIX%%型，使其能尽可能地进行正确的预测，我们可以获得单词的分布式表示。%%HIGHLIGHT%% ==CBOW模型的输入是上下文。== %%POSTFIX%%这个上下文用['you', 'goodbye'] 这样的单词列表*
>%%LINK%%[[#^una2zbiesy|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^una2zbiesy


>%%
>```annotation-json
>{"created":"2025-08-15T04:42:21.031Z","text":"CBOW计算结束后使用的东西。","updated":"2025-08-15T04:42:21.031Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":82866,"end":82888},{"type":"TextQuoteSelector","exact":"权重Win的各行保存着各个单词的分布式表示。","prefix":"logoodbye(7×3)sayandi.W如图3-10所示，","suffix":"通过反复学习，不断更新各个单词的分布式表示，以正确地从上下文预测"}]}]}
>```
>%%
>*%%PREFIX%%logoodbye(7×3)sayandi.W如图3-10所示，%%HIGHLIGHT%% ==权重Win的各行保存着各个单词的分布式表示。== %%POSTFIX%%通过反复学习，不断更新各个单词的分布式表示，以正确地从上下文预测*
>%%LINK%%[[#^jkgqq38973a|show annotation]]
>%%COMMENT%%
>CBOW计算结束后使用的东西。
>%%TAGS%%
>
^jkgqq38973a


>%%
>```annotation-json
>{"created":"2025-08-15T16:44:46.975Z","updated":"2025-08-15T16:44:46.975Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":85137,"end":85147},{"type":"TextQuoteSelector","exact":"多类别分类的神经网络","prefix":"一下上述神经网络的学习。其实很简单，这里我们处理的模型是一个进行","suffix":"。因此，对其进行学习只是使用一下Softmax函数和交叉熵误差。"}]}]}
>```
>%%
>*%%PREFIX%%一下上述神经网络的学习。其实很简单，这里我们处理的模型是一个进行%%HIGHLIGHT%% ==多类别分类的神经网络== %%POSTFIX%%。因此，对其进行学习只是使用一下Softmax函数和交叉熵误差。*
>%%LINK%%[[#^ssyj6jqr5g|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ssyj6jqr5g


>%%
>```annotation-json
>{"created":"2025-08-15T16:44:50.658Z","updated":"2025-08-15T16:44:50.658Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":85163,"end":85178},{"type":"TextQuoteSelector","exact":"Softmax函数和交叉熵误差","prefix":"型是一个进行多类别分类的神经网络。因此，对其进行学习只是使用一下","suffix":"。首先，使用Softmax函数将得分转化为概率，再求这些概率和监"}]}]}
>```
>%%
>*%%PREFIX%%型是一个进行多类别分类的神经网络。因此，对其进行学习只是使用一下%%HIGHLIGHT%% ==Softmax函数和交叉熵误差== %%POSTFIX%%。首先，使用Softmax函数将得分转化为概率，再求这些概率和监*
>%%LINK%%[[#^lw5yqnv0mnh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^lw5yqnv0mnh


>%%
>```annotation-json
>{"created":"2025-08-19T05:37:27.738Z","text":"最受欢迎的分布式选择。特别是skip-gram模型。","updated":"2025-08-19T05:37:27.738Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":86203,"end":86212},{"type":"TextQuoteSelector","exact":"只使用输入侧的权重","prefix":"最终应该使用哪个权重作为单词的分布式表示呢？这里有三个选项。A.","suffix":"B.只使用输出侧的权重C.同时使用两个权重方案A和方案B只使用其"}]}]}
>```
>%%
>*%%PREFIX%%最终应该使用哪个权重作为单词的分布式表示呢？这里有三个选项。A.%%HIGHLIGHT%% ==只使用输入侧的权重== %%POSTFIX%%B.只使用输出侧的权重C.同时使用两个权重方案A和方案B只使用其*
>%%LINK%%[[#^3jr4cdlw8zl|show annotation]]
>%%COMMENT%%
>最受欢迎的分布式选择。特别是skip-gram模型。
>%%TAGS%%
>#分布式
^3jr4cdlw8zl


>%%
>```annotation-json
>{"created":"2025-08-19T06:25:34.030Z","updated":"2025-08-19T06:25:34.030Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":86662,"end":86714},{"type":"TextQuoteSelector","exact":"word2vec中使用的神经网络的输入是上下文，它的正确解标签是被这些上下文包围在中间的单词，即目标词。","prefix":"这个只有一句话的语料库为例进行说明。3.3.1  上下文和目标词","suffix":"也就是说，我们要做的事情是，当向神经网络输入上下文时，使目标词出"}]}]}
>```
>%%
>*%%PREFIX%%这个只有一句话的语料库为例进行说明。3.3.1  上下文和目标词%%HIGHLIGHT%% ==word2vec中使用的神经网络的输入是上下文，它的正确解标签是被这些上下文包围在中间的单词，即目标词。== %%POSTFIX%%也就是说，我们要做的事情是，当向神经网络输入上下文时，使目标词出*
>%%LINK%%[[#^d5d21o2ov67|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^d5d21o2ov67


>%%
>```annotation-json
>{"created":"2025-08-19T07:11:50.045Z","updated":"2025-08-19T07:11:50.045Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":99193,"end":99241},{"type":"TextQuoteSelector","exact":"实际上，有研究表明，就单词相似性的定量评价而言，基于推理的方法和基于计数的方法难分上下[25]。","prefix":"常见的误解，那就是基于推理的方法在准确度方面优于基于计数的方法。","suffix":"2014年发表的题为“Don’t count, predict!"}]}]}
>```
>%%
>*%%PREFIX%%常见的误解，那就是基于推理的方法在准确度方面优于基于计数的方法。%%HIGHLIGHT%% ==实际上，有研究表明，就单词相似性的定量评价而言，基于推理的方法和基于计数的方法难分上下[25]。== %%POSTFIX%%2014年发表的题为“Don’t count, predict!*
>%%LINK%%[[#^6jnziuilg77|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^6jnziuilg77


>%%
>```annotation-json
>{"created":"2025-08-19T07:13:39.355Z","text":"Word2Vec (SGNS) 虽然看起来是通过神经网络训练得到的，但数学上等价于在做一个经过调整的词-上下文共现矩阵的分解。","updated":"2025-08-19T07:13:39.355Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":99435,"end":99527},{"type":"TextQuoteSelector","exact":"使用了skip-gram和下一章介绍的Negative Sampling的模型被证明与对整个语料库的共现矩阵（实际上会对矩阵进行一定的修改）进行特殊矩阵分解的方法具有相同的作用[26]","prefix":"要的事实是，基于推理的方法和基于计数的方法存在关联性。具体地说，","suffix":"。换句话说，这两个方法论（在某些条件下）是“相通”的。此外，在w"}]}]}
>```
>%%
>*%%PREFIX%%要的事实是，基于推理的方法和基于计数的方法存在关联性。具体地说，%%HIGHLIGHT%% ==使用了skip-gram和下一章介绍的Negative Sampling的模型被证明与对整个语料库的共现矩阵（实际上会对矩阵进行一定的修改）进行特殊矩阵分解的方法具有相同的作用[26]== %%POSTFIX%%。换句话说，这两个方法论（在某些条件下）是“相通”的。此外，在w*
>%%LINK%%[[#^r002qeuj75h|show annotation]]
>%%COMMENT%%
>Word2Vec (SGNS) 虽然看起来是通过神经网络训练得到的，但数学上等价于在做一个经过调整的词-上下文共现矩阵的分解。
>%%TAGS%%
>
^r002qeuj75h


>%%
>```annotation-json
>{"created":"2025-08-19T09:06:38.421Z","updated":"2025-08-19T09:06:38.421Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":101800,"end":101827},{"type":"TextQuoteSelector","exact":"随着词汇量的增加，one-hot表示的向量大小也会增加","prefix":"e-hot表示有关。这是因为我们用one-hot表示来处理单词，","suffix":"。比如，在词汇量有100万个的情况下，仅one-hot表示本身就"}]}]}
>```
>%%
>*%%PREFIX%%e-hot表示有关。这是因为我们用one-hot表示来处理单词，%%HIGHLIGHT%% ==随着词汇量的增加，one-hot表示的向量大小也会增加== %%POSTFIX%%。比如，在词汇量有100万个的情况下，仅one-hot表示本身就*
>%%LINK%%[[#^77y1reihalc|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^77y1reihalc


>%%
>```annotation-json
>{"created":"2025-08-19T09:12:51.540Z","updated":"2025-08-19T09:12:51.540Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":102586,"end":102611},{"type":"TextQuoteSelector","exact":"图4-3中所做的无非是将矩阵的某个特定的行取出来。","prefix":"也会是100万，我们需要计算这个巨大向量和权重矩阵的乘积。但是，","suffix":"因此，直觉上将单词转化为one-hot向量的处理和MatMul层"}]}]}
>```
>%%
>*%%PREFIX%%也会是100万，我们需要计算这个巨大向量和权重矩阵的乘积。但是，%%HIGHLIGHT%% ==图4-3中所做的无非是将矩阵的某个特定的行取出来。== %%POSTFIX%%因此，直觉上将单词转化为one-hot向量的处理和MatMul层*
>%%LINK%%[[#^qwlpolunqr|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^qwlpolunqr


>%%
>```annotation-json
>{"created":"2025-08-19T09:20:07.617Z","updated":"2025-08-19T09:20:07.617Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":104020,"end":104070},{"type":"TextQuoteSelector","exact":"Embedding层的正向传播只是从权重矩阵W 中提取特定的行，并将该特定行的神经元原样传给下一层。","prefix":"式保存需要提取的行的索引（单词ID）。接下来，我们考虑反向传播。","suffix":"因此，在反向传播时，从上一层（输出侧的层）传过来的梯度将原样传给"}]}]}
>```
>%%
>*%%PREFIX%%式保存需要提取的行的索引（单词ID）。接下来，我们考虑反向传播。%%HIGHLIGHT%% ==Embedding层的正向传播只是从权重矩阵W 中提取特定的行，并将该特定行的神经元原样传给下一层。== %%POSTFIX%%因此，在反向传播时，从上一层（输出侧的层）传过来的梯度将原样传给*
>%%LINK%%[[#^22ezadfotq9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^22ezadfotq9


>%%
>```annotation-json
>{"created":"2025-08-25T08:41:50.805Z","updated":"2025-08-25T08:41:50.805Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":106218,"end":106273},{"type":"TextQuoteSelector","exact":"此时，在以下两个地方需要很多计算时间。• 中间层的神经元和权重矩阵（Wout）的乘积• Softmax层的计算","prefix":"层，节省了输入层中不必要的计算。剩下的问题就是中间层之后的处理。","suffix":"4.2  word2vec的改进②  139第1个问题在于巨大的"}]}]}
>```
>%%
>*%%PREFIX%%层，节省了输入层中不必要的计算。剩下的问题就是中间层之后的处理。%%HIGHLIGHT%% ==此时，在以下两个地方需要很多计算时间。• 中间层的神经元和权重矩阵（Wout）的乘积• Softmax层的计算== %%POSTFIX%%4.2  word2vec的改进②  139第1个问题在于巨大的*
>%%LINK%%[[#^7zng1rp5ewy|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7zng1rp5ewy


>%%
>```annotation-json
>{"created":"2025-08-25T08:43:22.902Z","updated":"2025-08-25T08:43:22.902Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":106690,"end":106783},{"type":"TextQuoteSelector","exact":"这个方法的关键思想在于二分类（binary classification），更准确地说，是用二分类拟合多分类（multiclass classification），这是理解负采样的重点。","prefix":"计算。4.2.2  从多分类到二分类下面，我们来解释一下负采样。","suffix":"到目前为止，我们处理的都是多分类问题。拿刚才的例子来说，我们把它"}]}]}
>```
>%%
>*%%PREFIX%%计算。4.2.2  从多分类到二分类下面，我们来解释一下负采样。%%HIGHLIGHT%% ==这个方法的关键思想在于二分类（binary classification），更准确地说，是用二分类拟合多分类（multiclass classification），这是理解负采样的重点。== %%POSTFIX%%到目前为止，我们处理的都是多分类问题。拿刚才的例子来说，我们把它*
>%%LINK%%[[#^h55ef5b4gh4|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^h55ef5b4gh4


>%%
>```annotation-json
>{"created":"2025-08-28T08:08:30.142Z","updated":"2025-08-28T08:08:30.142Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":112930,"end":112970},{"type":"TextQuoteSelector","exact":"作为一种近似方法，我们将选择若干个（5个或者10个）负例（如何选择将在下文介绍）","prefix":"无法处理（更何况本章的目的本来就是解决词汇量增加的问题）。为此，","suffix":"。也就是说，只使用少数负例。这就是负采样方法的含义。总而言之，负"}]}]}
>```
>%%
>*%%PREFIX%%无法处理（更何况本章的目的本来就是解决词汇量增加的问题）。为此，%%HIGHLIGHT%% ==作为一种近似方法，我们将选择若干个（5个或者10个）负例（如何选择将在下文介绍）== %%POSTFIX%%。也就是说，只使用少数负例。这就是负采样方法的含义。总而言之，负*
>%%LINK%%[[#^50bzi29gaft|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^50bzi29gaft


>%%
>```annotation-json
>{"created":"2025-08-28T08:09:10.173Z","text":"采样的意思是从不是期望的结果中选出少量例子，作为loss一起计算。\n","updated":"2025-08-28T08:09:10.173Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":113001,"end":113088},{"type":"TextQuoteSelector","exact":"负采样方法既可以求将正例作为目标词时的损失，同时也可以采样（选出）若干个负例，对这些负例求损失。然后，将这些数据（正例和采样出来的负例）的损失加起来，将其结果作为最终的损失。","prefix":"）。也就是说，只使用少数负例。这就是负采样方法的含义。总而言之，","suffix":"下面，让我们结合具体的例子来说明。这里使用与之前相同的例子（正例"}]}]}
>```
>%%
>*%%PREFIX%%）。也就是说，只使用少数负例。这就是负采样方法的含义。总而言之，%%HIGHLIGHT%% ==负采样方法既可以求将正例作为目标词时的损失，同时也可以采样（选出）若干个负例，对这些负例求损失。然后，将这些数据（正例和采样出来的负例）的损失加起来，将其结果作为最终的损失。== %%POSTFIX%%下面，让我们结合具体的例子来说明。这里使用与之前相同的例子（正例*
>%%LINK%%[[#^w9tpew4wlia|show annotation]]
>%%COMMENT%%
>采样的意思是从不是期望的结果中选出少量例子，作为loss一起计算。
>
>%%TAGS%%
>
^w9tpew4wlia


>%%
>```annotation-json
>{"created":"2025-08-28T08:10:08.782Z","updated":"2025-08-28T08:10:08.782Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":113613,"end":113638},{"type":"TextQuoteSelector","exact":"基于语料库的统计数据进行采样的方法比随机抽样要好。","prefix":"6  负采样的采样方法下面我们来看一下如何抽取负例。关于这一点，","suffix":"具体来说，就是让语料库中经常出现的单词容易被抽到，让语料库中不经"}]}]}
>```
>%%
>*%%PREFIX%%6  负采样的采样方法下面我们来看一下如何抽取负例。关于这一点，%%HIGHLIGHT%% ==基于语料库的统计数据进行采样的方法比随机抽样要好。== %%POSTFIX%%具体来说，就是让语料库中经常出现的单词容易被抽到，让语料库中不经*
>%%LINK%%[[#^9y1gcrpmw9b|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^9y1gcrpmw9b


>%%
>```annotation-json
>{"created":"2025-08-28T08:13:50.035Z","updated":"2025-08-28T08:13:50.035Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":114693,"end":114768},{"type":"TextQuoteSelector","exact":"np.random.choice() 可以用于随机抽样。如果指定size 参数，将执行多次采样。如果指定replace=False，将进行无放回采样。","prefix":"dom.choice(words, p=p)'you'如上所示，","suffix":"通过给参数p 指定表示概率分布的列表，将进行基于概率分布的采样。"}]}]}
>```
>%%
>*%%PREFIX%%dom.choice(words, p=p)'you'如上所示，%%HIGHLIGHT%% ==np.random.choice() 可以用于随机抽样。如果指定size 参数，将执行多次采样。如果指定replace=False，将进行无放回采样。== %%POSTFIX%%通过给参数p 指定表示概率分布的列表，将进行基于概率分布的采样。*
>%%LINK%%[[#^aap38imbwjo|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^aap38imbwjo


>%%
>```annotation-json
>{"created":"2025-08-28T08:14:32.163Z","updated":"2025-08-28T08:14:32.163Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":115058,"end":115081},{"type":"TextQuoteSelector","exact":"通过取0.75次方，低频单词的概率将稍微变高。","prefix":"行式(4.4)的变换呢？这是为了防止低频单词被忽略。更准确地说，","suffix":"我们来看一个具体例子，如下所示。>>> p = [0.7, 0."}]}]}
>```
>%%
>*%%PREFIX%%行式(4.4)的变换呢？这是为了防止低频单词被忽略。更准确地说，%%HIGHLIGHT%% ==通过取0.75次方，低频单词的概率将稍微变高。== %%POSTFIX%%我们来看一个具体例子，如下所示。>>> p = [0.7, 0.*
>%%LINK%%[[#^xxo7r3gvn5|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^xxo7r3gvn5


>%%
>```annotation-json
>{"created":"2025-08-28T15:40:02.078Z","updated":"2025-08-28T15:40:02.078Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":127162,"end":127227},{"type":"TextQuoteSelector","exact":"而是先在大规模语料库（Wikipedia、Google News等文本数据）上学习，然后将学习好的分布式表示应用于某个单独的任务。","prefix":"务时，一般不会使用word2vec从零开始学习单词的分布式表示，","suffix":"比如，在文本分类、文本聚类、词性标注和情感分析等自然语言处理任务"}]}]}
>```
>%%
>*%%PREFIX%%务时，一般不会使用word2vec从零开始学习单词的分布式表示，%%HIGHLIGHT%% ==而是先在大规模语料库（Wikipedia、Google News等文本数据）上学习，然后将学习好的分布式表示应用于某个单独的任务。== %%POSTFIX%%比如，在文本分类、文本聚类、词性标注和情感分析等自然语言处理任务*
>%%LINK%%[[#^wfmotyrtgu|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^wfmotyrtgu


>%%
>```annotation-json
>{"created":"2025-08-28T15:40:18.658Z","updated":"2025-08-28T15:40:18.658Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":127430,"end":127455},{"type":"TextQuoteSelector","exact":"把文档的各个单词转化为分布式表示，然后求它们的总和","prefix":"档转化为固定长度的向量，相关研究已经进行了很多，最简单的方法是，","suffix":"。这是一种被称为bag-of-words的不考虑单词顺序的模型（"}]}]}
>```
>%%
>*%%PREFIX%%档转化为固定长度的向量，相关研究已经进行了很多，最简单的方法是，%%HIGHLIGHT%% ==把文档的各个单词转化为分布式表示，然后求它们的总和== %%POSTFIX%%。这是一种被称为bag-of-words的不考虑单词顺序的模型（*
>%%LINK%%[[#^qyrwqokvl4|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^qyrwqokvl4


>%%
>```annotation-json
>{"created":"2025-08-28T15:40:36.408Z","updated":"2025-08-28T15:40:36.408Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":127460,"end":127490},{"type":"TextQuoteSelector","exact":"被称为bag-of-words的不考虑单词顺序的模型（思想）","prefix":"是，把文档的各个单词转化为分布式表示，然后求它们的总和。这是一种","suffix":"。此外，使用即将在第5章中说明的循环神经网络，可以以更加优美的方"}]}]}
>```
>%%
>*%%PREFIX%%是，把文档的各个单词转化为分布式表示，然后求它们的总和。这是一种%%HIGHLIGHT%% ==被称为bag-of-words的不考虑单词顺序的模型（思想）== %%POSTFIX%%。此外，使用即将在第5章中说明的循环神经网络，可以以更加优美的方*
>%%LINK%%[[#^guw7juv0glu|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^guw7juv0glu


>%%
>```annotation-json
>{"created":"2025-08-28T15:41:53.111Z","updated":"2025-08-28T15:41:53.111Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":129080,"end":129092},{"type":"TextQuoteSelector","exact":"“相似度”和“类推问题”","prefix":"布式表示的评价往往与实际应用分开进行。此时，经常使用的评价指标有","suffix":"。单词相似度的评价通常使用人工创建的单词相似度评价集来评估。比如"}]}]}
>```
>%%
>*%%PREFIX%%布式表示的评价往往与实际应用分开进行。此时，经常使用的评价指标有%%HIGHLIGHT%% ==“相似度”和“类推问题”== %%POSTFIX%%。单词相似度的评价通常使用人工创建的单词相似度评价集来评估。比如*
>%%LINK%%[[#^bcy336vgu84|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^bcy336vgu84


>%%
>```annotation-json
>{"created":"2025-08-28T15:41:58.419Z","updated":"2025-08-28T15:41:58.419Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":129093,"end":129121},{"type":"TextQuoteSelector","exact":"单词相似度的评价通常使用人工创建的单词相似度评价集来评估","prefix":"用分开进行。此时，经常使用的评价指标有“相似度”和“类推问题”。","suffix":"。比如，cat和animal的相似度是8，cat和car的相似度"}]}]}
>```
>%%
>*%%PREFIX%%用分开进行。此时，经常使用的评价指标有“相似度”和“类推问题”。%%HIGHLIGHT%% ==单词相似度的评价通常使用人工创建的单词相似度评价集来评估== %%POSTFIX%%。比如，cat和animal的相似度是8，cat和car的相似度*
>%%LINK%%[[#^zxdfq9co6d|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^zxdfq9co6d


>%%
>```annotation-json
>{"created":"2025-08-28T15:56:58.066Z","updated":"2025-08-28T15:56:58.066Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":133719,"end":133758},{"type":"TextQuoteSelector","exact":"这里需要注意的是，这个后验概率是以目标词左侧的全部单词为上下文（条件）时的概率","prefix":")可以表示为后验概率的乘积P(wt|w1,···,wt−1)。","suffix":"，如图5-3所示。  第5章 RNN178......w1 wt"}]}]}
>```
>%%
>*%%PREFIX%%)可以表示为后验概率的乘积P(wt|w1,···,wt−1)。%%HIGHLIGHT%% ==这里需要注意的是，这个后验概率是以目标词左侧的全部单词为上下文（条件）时的概率== %%POSTFIX%%，如图5-3所示。  第5章 RNN178......w1 wt*
>%%LINK%%[[#^7tadu19g2s|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7tadu19g2s


>%%
>```annotation-json
>{"created":"2025-08-28T15:58:22.822Z","updated":"2025-08-28T15:58:22.822Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":133860,"end":133925},{"type":"TextQuoteSelector","exact":"我们的目标是求P(wt|w1,···,wt−1)这个概率。如果能计算出这个概率，就能求得语言模型的联合概率P(w1,···,wm)","prefix":"则第t个单词左侧的全部单词构成上下文（条件）这里我们来总结一下，","suffix":"。由 P(wt|w1,···,wt−1)表示的模型称为条件语言模"}]}]}
>```
>%%
>*%%PREFIX%%则第t个单词左侧的全部单词构成上下文（条件）这里我们来总结一下，%%HIGHLIGHT%% ==我们的目标是求P(wt|w1,···,wt−1)这个概率。如果能计算出这个概率，就能求得语言模型的联合概率P(w1,···,wm)== %%POSTFIX%%。由 P(wt|w1,···,wt−1)表示的模型称为条件语言模*
>%%LINK%%[[#^zqqfn2929oh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^zqqfn2929oh


>%%
>```annotation-json
>{"created":"2025-08-28T16:02:40.125Z","text":"太小不准，太大，也没有注意顺序问题。\n\n窗口为2的时候，上一个单词和下一个单词的顺序无关，重要的是在这两个单词中间","updated":"2025-08-28T16:02:40.125Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":134803,"end":134847},{"type":"TextQuoteSelector","exact":"CBOW模型的上下文大小可以任意设定，但是CBOW模型还存在忽视了上下文中单词顺序的问题","prefix":"OW模型的上下文大小（比如变为20或30）来解决此问题呢？的确，","suffix":"。CBOW是 Continuous Bag-Of-Words的 "}]}]}
>```
>%%
>*%%PREFIX%%OW模型的上下文大小（比如变为20或30）来解决此问题呢？的确，%%HIGHLIGHT%% ==CBOW模型的上下文大小可以任意设定，但是CBOW模型还存在忽视了上下文中单词顺序的问题== %%POSTFIX%%。CBOW是 Continuous Bag-Of-Words的*
>%%LINK%%[[#^6qed9dsnmne|show annotation]]
>%%COMMENT%%
>太小不准，太大，也没有注意顺序问题。
>
>窗口为2的时候，上一个单词和下一个单词的顺序无关，重要的是在这两个单词中间
>%%TAGS%%
>
^6qed9dsnmne


>%%
>```annotation-json
>{"created":"2025-08-29T04:31:12.488Z","updated":"2025-08-29T04:31:12.488Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":137272,"end":137331},{"type":"TextQuoteSelector","exact":"各个时刻的RNN层接收传给该层的输入和前一个RNN层的输出，然后据此计算当前时刻的输出，此时进行的计算可以用下式表示：","prefix":"的单词”或者“时刻t的RNN层”这样的表述。由图5-8可以看出，","suffix":" ht=tanh(ht−1Wh+xtWx+b) (5.9)首先说"}]}]}
>```
>%%
>*%%PREFIX%%的单词”或者“时刻t的RNN层”这样的表述。由图5-8可以看出，%%HIGHLIGHT%% ==各个时刻的RNN层接收传给该层的输入和前一个RNN层的输出，然后据此计算当前时刻的输出，此时进行的计算可以用下式表示：== %%POSTFIX%%ht=tanh(ht−1Wh+xtWx+b) (5.9)首先说*
>%%LINK%%[[#^f8wydj026ow|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^f8wydj026ow


>%%
>```annotation-json
>{"created":"2025-08-29T04:31:17.178Z","updated":"2025-08-29T04:31:17.178Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":137332,"end":137352},{"type":"TextQuoteSelector","exact":"ht=tanh(ht−1Wh+xtWx+","prefix":"出，然后据此计算当前时刻的输出，此时进行的计算可以用下式表示： ","suffix":"b) (5.9)首先说明一下式(5.9)中的符号。RNN有两个权"}]}]}
>```
>%%
>*%%PREFIX%%出，然后据此计算当前时刻的输出，此时进行的计算可以用下式表示：%%HIGHLIGHT%% ==ht=tanh(ht−1Wh+xtWx+== %%POSTFIX%%b) (5.9)首先说明一下式(5.9)中的符号。RNN有两个权*
>%%LINK%%[[#^39wyproy13m|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^39wyproy13m


>%%
>```annotation-json
>{"created":"2025-08-29T04:36:19.931Z","updated":"2025-08-29T04:36:19.931Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":138667,"end":138726},{"type":"TextQuoteSelector","exact":"在Truncated BPTT中，网络连接被截断，但严格地讲，只是网络的反向传播的连接被截断，正向传播的连接依然被维持","prefix":"Truncated BPTT是指按适当长度截断的误差反向传播法。","suffix":"，这一点很重要。也就是说，正向传播的信息没有中断地传播。与此相对"}]}]}
>```
>%%
>*%%PREFIX%%Truncated BPTT是指按适当长度截断的误差反向传播法。%%HIGHLIGHT%% ==在Truncated BPTT中，网络连接被截断，但严格地讲，只是网络的反向传播的连接被截断，正向传播的连接依然被维持== %%POSTFIX%%，这一点很重要。也就是说，正向传播的信息没有中断地传播。与此相对*
>%%LINK%%[[#^f6ml17wz1kp|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^f6ml17wz1kp


>%%
>```annotation-json
>{"created":"2025-08-29T04:39:28.377Z","updated":"2025-08-29T04:39:28.377Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":139398,"end":139439},{"type":"TextQuoteSelector","exact":"在进行RNN的学习时，必须考虑到正向传播之间是有关联的，这意味着必须按顺序输入数据","prefix":"的是，虽然反向传播的连接会被截断，但是正向传播的连接不会。因此，","suffix":"。下面，我们来说明什么是按顺序输入数据。我们之前看到的神经网络在"}]}]}
>```
>%%
>*%%PREFIX%%的是，虽然反向传播的连接会被截断，但是正向传播的连接不会。因此，%%HIGHLIGHT%% ==在进行RNN的学习时，必须考虑到正向传播之间是有关联的，这意味着必须按顺序输入数据== %%POSTFIX%%。下面，我们来说明什么是按顺序输入数据。我们之前看到的神经网络在*
>%%LINK%%[[#^zb30a98d0t|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^zb30a98d0t


>%%
>```annotation-json
>{"created":"2025-08-29T04:46:33.854Z","text":"Truncated BPTT的mini-batch学习。\n\n需要按照批次顺序输入，每次新的batch进行偏移。","updated":"2025-08-29T04:46:33.854Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":141241,"end":141249},{"type":"TextQuoteSelector","exact":"如图5-15所示","prefix":"11 x519ឦ⁎⮱す1 ٰ͗㉍ឦ⁎⮱す2 ٰ͗㉍䔈㵹႓΍⮱䶧Ꮌ","suffix":"，批次的第1个元素是x0,···,x9，批次的第2个元素是x50"}]}]}
>```
>%%
>*%%PREFIX%%11 x519ឦ⁎⮱す1 ٰ͗㉍ឦ⁎⮱す2 ٰ͗㉍䔈㵹႓΍⮱䶧Ꮌ%%HIGHLIGHT%% ==如图5-15所示== %%POSTFIX%%，批次的第1个元素是x0,···,x9，批次的第2个元素是x50*
>%%LINK%%[[#^7j1a9mnaf5e|show annotation]]
>%%COMMENT%%
>Truncated BPTT的mini-batch学习。
>
>需要按照批次顺序输入，每次新的batch进行偏移。
>%%TAGS%%
>
^7j1a9mnaf5e


>%%
>```annotation-json
>{"created":"2025-08-29T04:47:14.311Z","updated":"2025-08-29T04:47:14.311Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":141409,"end":141441},{"type":"TextQuoteSelector","exact":"此外，如果在按顺序输入数据的过程中遇到了结尾，则需要设法返回头部","prefix":"i-batch学习时，平移各批次输入数据的开始位置，按顺序输入。","suffix":"。如上所述，虽然Truncated BPTT的原理非常简单，但是"}]}]}
>```
>%%
>*%%PREFIX%%i-batch学习时，平移各批次输入数据的开始位置，按顺序输入。%%HIGHLIGHT%% ==此外，如果在按顺序输入数据的过程中遇到了结尾，则需要设法返回头部== %%POSTFIX%%。如上所述，虽然Truncated BPTT的原理非常简单，但是*
>%%LINK%%[[#^iw35ekcvj4n|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^iw35ekcvj4n


>%%
>```annotation-json
>{"created":"2025-08-29T05:04:31.290Z","updated":"2025-08-29T05:04:31.290Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":144444,"end":144467},{"type":"TextQuoteSelector","exact":"Time RNN层是T个RNN层连接起来的网络","prefix":"1,···,xT−1)hsxsTime RNN由图5-21可知，","suffix":"。我们将这个网络实现为Time RNN层。这里，RNN层的隐藏状"}]}]}
>```
>%%
>*%%PREFIX%%1,···,xT−1)hsxsTime RNN由图5-21可知，%%HIGHLIGHT%% ==Time RNN层是T个RNN层连接起来的网络== %%POSTFIX%%。我们将这个网络实现为Time RNN层。这里，RNN层的隐藏状*
>%%LINK%%[[#^vvz0orbaq49|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^vvz0orbaq49
