---
annotation-target: file:/Users/zhou-wentong/Documents/books/深度学习进阶-自然语言处理.pdf
---


>%%
>```annotation-json
>{"created":"2025-08-13T00:52:14.404Z","text":"Githu仓库\n","updated":"2025-08-13T00:52:14.404Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":17756,"end":17817},{"type":"TextQuoteSelector","exact":"https://github.com/oreilly-japan/deep-learning-from-scratch-2","prefix":"ly.co.jp/books/9784873118369（日语）","suffix":"  前言xx关于O’Reilly的其他信息，可以访问下面的O’R"}]}]}
>```
>%%
>*%%PREFIX%%ly.co.jp/books/9784873118369（日语）%%HIGHLIGHT%% ==https://github.com/oreilly-japan/deep-learning-from-scratch-2== %%POSTFIX%%前言xx关于O’Reilly的其他信息，可以访问下面的O’R*
>%%LINK%%[[#^fposwz6dju9|show annotation]]
>%%COMMENT%%
>Githu仓库
>
>%%TAGS%%
>#github, #code
^fposwz6dju9


>%%
>```annotation-json
>{"created":"2025-08-13T01:08:51.425Z","text":"多类别分类：有很多类别，但只属于其中某一类\n多标签：有很多类别，可能被分配多个标签（使用sigmoid）","updated":"2025-08-13T01:08:51.425Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":28019,"end":28068},{"type":"TextQuoteSelector","exact":"进行多类别分类的神经网络通常使用交叉熵误差（cross entropy error）作为损失函数。","prefix":"计算神经网络的损失要使用损失函数（loss function）。","suffix":"此时，交叉熵误差由神经网络输出的各类别的概率和监督标签求得。现在"}]}]}
>```
>%%
>*%%PREFIX%%计算神经网络的损失要使用损失函数（loss function）。%%HIGHLIGHT%% ==进行多类别分类的神经网络通常使用交叉熵误差（cross entropy error）作为损失函数。== %%POSTFIX%%此时，交叉熵误差由神经网络输出的各类别的概率和监督标签求得。现在*
>%%LINK%%[[#^ud1pfn0cuxq|show annotation]]
>%%COMMENT%%
>多类别分类：有很多类别，但只属于其中某一类
>多标签：有很多类别，可能被分配多个标签（使用sigmoid）
>%%TAGS%%
>#ce-loss
^ud1pfn0cuxq


>%%
>```annotation-json
>{"created":"2025-08-13T01:13:20.344Z","text":"交叉熵误差","updated":"2025-08-13T01:13:20.344Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":28743,"end":28760},{"type":"TextQuoteSelector","exact":"=−∑ktklogyk (1.7)","prefix":"，这个概率会被输入交叉熵误差。此时，交叉熵误差可由下式表示： L","suffix":"  第1章 神经网络的复习20这里，tk是对应于第k个类别的监督"}]}]}
>```
>%%
>*%%PREFIX%%，这个概率会被输入交叉熵误差。此时，交叉熵误差可由下式表示： L%%HIGHLIGHT%% ===−∑ktklogyk (1.7)== %%POSTFIX%%第1章 神经网络的复习20这里，tk是对应于第k个类别的监督*
>%%LINK%%[[#^xf713toupbi|show annotation]]
>%%COMMENT%%
>交叉熵误差
>%%TAGS%%
>
^xf713toupbi


>%%
>```annotation-json
>{"created":"2025-08-13T01:18:39.894Z","text":"有多个自变量时，分别计算因变量关于每个自变量的偏导，罗列到一起，就是梯度（gradient）。\n严格来说，和数学领域中的梯度略有区别。","updated":"2025-08-13T01:18:39.894Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":29964,"end":29987},{"type":"TextQuoteSelector","exact":"将关于向量各个元素的导数罗列到一起，就得到了梯","prefix":"∂L∂x1,∂L∂x2,...,∂L∂xn) (1.9)像这样，","suffix":"度（gradient）。另外，矩阵也可以像向量一样求梯度。假设W"}]}]}
>```
>%%
>*%%PREFIX%%∂L∂x1,∂L∂x2,...,∂L∂xn) (1.9)像这样，%%HIGHLIGHT%% ==将关于向量各个元素的导数罗列到一起，就得到了梯== %%POSTFIX%%度（gradient）。另外，矩阵也可以像向量一样求梯度。假设W*
>%%LINK%%[[#^gmqohl5bqpq|show annotation]]
>%%COMMENT%%
>有多个自变量时，分别计算因变量关于每个自变量的偏导，罗列到一起，就是梯度（gradient）。
>严格来说，和数学领域中的梯度略有区别。
>%%TAGS%%
>#梯度, #gradient
^gmqohl5bqpq


>%%
>```annotation-json
>{"created":"2025-08-13T01:18:13.974Z","text":"导数含义","updated":"2025-08-13T01:18:13.974Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":29646,"end":29683},{"type":"TextQuoteSelector","exact":"就是x的微小（严格地讲，“微小”为无限小）变化会导致y发生多大程度的变化。","prefix":"的导数记为 dydx。这个 dydx的意思是变化程度，具体来说，","suffix":"比如函数y=x2，其导数可以解析性地求出，即 dydx=2x。这"}]}]}
>```
>%%
>*%%PREFIX%%的导数记为 dydx。这个 dydx的意思是变化程度，具体来说，%%HIGHLIGHT%% ==就是x的微小（严格地讲，“微小”为无限小）变化会导致y发生多大程度的变化。== %%POSTFIX%%比如函数y=x2，其导数可以解析性地求出，即 dydx=2x。这*
>%%LINK%%[[#^4dxioxzs5we|show annotation]]
>%%COMMENT%%
>导数含义
>%%TAGS%%
>
^4dxioxzs5we


>%%
>```annotation-json
>{"created":"2025-08-13T01:20:10.692Z","text":"和数学中的梯度的区别\n","updated":"2025-08-13T01:20:10.692Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":30330,"end":30379},{"type":"TextQuoteSelector","exact":"数学中的梯度仅限于关于向量的导数。而在深度学习领域，一般也会定义关于矩阵和张量的导数，称为“梯度”","prefix":"现。严格地说，本书使用的“梯度”一词与数学中的“梯度”是不同的。","suffix":"。1.3.3  链式法则学习阶段的神经网络在给定学习数据后会输出"}]}]}
>```
>%%
>*%%PREFIX%%现。严格地说，本书使用的“梯度”一词与数学中的“梯度”是不同的。%%HIGHLIGHT%% ==数学中的梯度仅限于关于向量的导数。而在深度学习领域，一般也会定义关于矩阵和张量的导数，称为“梯度”== %%POSTFIX%%。1.3.3  链式法则学习阶段的神经网络在给定学习数据后会输出*
>%%LINK%%[[#^cot1wtvxc65|show annotation]]
>%%COMMENT%%
>和数学中的梯度的区别
>
>%%TAGS%%
>
^cot1wtvxc65


>%%
>```annotation-json
>{"created":"2025-08-13T07:40:49.862Z","text":"找到损失增加最多的方向，顺着这个方向反向更新就下降最快\n","updated":"2025-08-13T07:40:49.862Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":39329,"end":39383},{"type":"TextQuoteSelector","exact":"这个梯度指向当前的权重参数所处位置中损失增加最多的方向。因此，通过将参数向该梯度的反方向更新，可以降低损失。","prefix":"选择mini-batch数据，根据误差反向传播法获得权重的梯度。","suffix":"这就是梯度下降法（gradient descent）。之后，根据"}]}]}
>```
>%%
>*%%PREFIX%%选择mini-batch数据，根据误差反向传播法获得权重的梯度。%%HIGHLIGHT%% ==这个梯度指向当前的权重参数所处位置中损失增加最多的方向。因此，通过将参数向该梯度的反方向更新，可以降低损失。== %%POSTFIX%%这就是梯度下降法（gradient descent）。之后，根据*
>%%LINK%%[[#^8fqldro6ye|show annotation]]
>%%COMMENT%%
>找到损失增加最多的方向，顺着这个方向反向更新就下降最快
>
>%%TAGS%%
>
^8fqldro6ye


>%%
>```annotation-json
>{"created":"2025-08-13T01:12:32.484Z","text":"所有类别的输出和为1，可以解释为概率。\n","updated":"2025-08-13T01:12:32.484Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":28639,"end":28666},{"type":"TextQuoteSelector","exact":"Softmax函数输出的各个元素是0.0～1.0的实数","prefix":"数的分子是得分sk的指数函数，分母是所有输入信号的指数函数的和。","suffix":"。另外，如果将这些元素全部加起来，则和为1。因此，Softmax"}]}]}
>```
>%%
>*%%PREFIX%%数的分子是得分sk的指数函数，分母是所有输入信号的指数函数的和。%%HIGHLIGHT%% ==Softmax函数输出的各个元素是0.0～1.0的实数== %%POSTFIX%%。另外，如果将这些元素全部加起来，则和为1。因此，Softmax*
>%%LINK%%[[#^2kenz90y9vo|show annotation]]
>%%COMMENT%%
>所有类别的输出和为1，可以解释为概率。
>
>%%TAGS%%
>
^2kenz90y9vo


>%%
>```annotation-json
>{"created":"2025-08-13T08:24:50.520Z","updated":"2025-08-13T08:24:50.520Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":52298,"end":52342},{"type":"TextQuoteSelector","exact":"在同义词词典中，具有相同含义的单词（同义词）或含义类似的单词（近义词）被归类到同一个组中","prefix":"常规词典，而是一种被称为同义词词典（thesaurus）的词典。","suffix":"。比如，使用同义词词典，我们可以知道car的同义词有automo"}]}]}
>```
>%%
>*%%PREFIX%%常规词典，而是一种被称为同义词词典（thesaurus）的词典。%%HIGHLIGHT%% ==在同义词词典中，具有相同含义的单词（同义词）或含义类似的单词（近义词）被归类到同一个组中== %%POSTFIX%%。比如，使用同义词词典，我们可以知道car的同义词有automo*
>%%LINK%%[[#^hbevd1548y7|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#同义词词典
^hbevd1548y7


>%%
>```annotation-json
>{"created":"2025-08-13T08:28:07.263Z","updated":"2025-08-13T08:28:07.263Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":53099,"end":53149},{"type":"TextQuoteSelector","exact":"使用WordNet，可以获得单词的近义词，或者利用单词网络。使用单词网络，可以计算单词之间的相似度。","prefix":"同义词词典，迄今已用于许多研究，并活跃于各种自然语言处理应用中。","suffix":"这里，我们不对WordNet进行详细说明，对WordNet的Py"}]}]}
>```
>%%
>*%%PREFIX%%同义词词典，迄今已用于许多研究，并活跃于各种自然语言处理应用中。%%HIGHLIGHT%% ==使用WordNet，可以获得单词的近义词，或者利用单词网络。使用单词网络，可以计算单词之间的相似度。== %%POSTFIX%%这里，我们不对WordNet进行详细说明，对WordNet的Py*
>%%LINK%%[[#^j3qnwbwh5og|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#WordNet
^j3qnwbwh5og


>%%
>```annotation-json
>{"created":"2025-08-13T08:32:27.399Z","text":"是基于计数的方法，为NLP研究和应而专门手机的数据。","updated":"2025-08-13T08:32:27.399Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":54307,"end":54318},{"type":"TextQuoteSelector","exact":"语料库（corpus）","prefix":" 632.3 基于计数的方法从介绍基于计数的方法开始，我们将使用","suffix":"。简而言之，语料库就是大量的文本数据。不过，语料库并不是胡乱收集"}]}]}
>```
>%%
>*%%PREFIX%%632.3 基于计数的方法从介绍基于计数的方法开始，我们将使用%%HIGHLIGHT%% ==语料库（corpus）== %%POSTFIX%%。简而言之，语料库就是大量的文本数据。不过，语料库并不是胡乱收集*
>%%LINK%%[[#^asdmjt6lqmu|show annotation]]
>%%COMMENT%%
>是基于计数的方法，为NLP研究和应而专门手机的数据。
>%%TAGS%%
>#语料库, #corpus
^asdmjt6lqmu


>%%
>```annotation-json
>{"created":"2025-08-13T08:36:48.414Z","text":"语料预处理代码（英文文本）","updated":"2025-08-13T08:36:48.414Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":56745,"end":56753},{"type":"TextQuoteSelector","exact":"语料库的准备工作","prefix":"  第2章 自然语言和单词的分布式表示66至此，我们就完成了利用","suffix":"。现在，我们将上述一系列处理实现为preprocess() 函数"}]}]}
>```
>%%
>*%%PREFIX%%第2章 自然语言和单词的分布式表示66至此，我们就完成了利用%%HIGHLIGHT%% ==语料库的准备工作== %%POSTFIX%%。现在，我们将上述一系列处理实现为preprocess() 函数*
>%%LINK%%[[#^3uevkyzj5pl|show annotation]]
>%%COMMENT%%
>语料预处理代码（英文文本）
>%%TAGS%%
>
^3uevkyzj5pl


>%%
>```annotation-json
>{"created":"2025-08-13T08:39:59.030Z","text":"有趣","updated":"2025-08-13T08:39:59.030Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":58250,"end":58309},{"type":"TextQuoteSelector","exact":"这个想法就是“某个单词的含义由它周围的单词形成”，称为分布式假设（distributional hypothesis）","prefix":"看一下这些研究，就会发现几乎所有的重要方法都基于一个简单的想法，","suffix":"。许多用向量表示单词的近期研究也基于该假设。分布式假设所表达的理"}]}]}
>```
>%%
>*%%PREFIX%%看一下这些研究，就会发现几乎所有的重要方法都基于一个简单的想法，%%HIGHLIGHT%% ==这个想法就是“某个单词的含义由它周围的单词形成”，称为分布式假设（distributional hypothesis）== %%POSTFIX%%。许多用向量表示单词的近期研究也基于该假设。分布式假设所表达的理*
>%%LINK%%[[#^fynp0i8zg1|show annotation]]
>%%COMMENT%%
>有趣
>%%TAGS%%
>#分布式假设, #distributional hypothesis
^fynp0i8zg1


>%%
>```annotation-json
>{"created":"2025-08-13T08:48:12.441Z","updated":"2025-08-13T08:48:12.441Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":61817,"end":61841},{"type":"TextQuoteSelector","exact":"余弦相似度（cosine similarity）","prefix":"虽然除此之外还有很多方法，但是在测量单词的向量表示的相似度方面，","suffix":"是很常用的。设有x=(x1,x2,x3,···,xn)和y=(y"}]}]}
>```
>%%
>*%%PREFIX%%虽然除此之外还有很多方法，但是在测量单词的向量表示的相似度方面，%%HIGHLIGHT%% ==余弦相似度（cosine similarity）== %%POSTFIX%%是很常用的。设有x=(x1,x2,x3,···,xn)和y=(y*
>%%LINK%%[[#^6ntbkl9jwmn|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#余弦相似度
^6ntbkl9jwmn


>%%
>```annotation-json
>{"created":"2025-08-13T08:39:14.310Z","text":"单词的分布表示，就是处理成可以计算机处理的向量。","updated":"2025-08-13T08:39:14.310Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":58026,"end":58048},{"type":"TextQuoteSelector","exact":"单词的分布式表示将单词表示为固定长度的向量。","prefix":"确把握单词含义的向量表示。在自然语言处理领域，这称为分布式表示。","suffix":"这种向量的特征在于它是用密集向量表示的。密集向量的意思是，向量的"}]}]}
>```
>%%
>*%%PREFIX%%确把握单词含义的向量表示。在自然语言处理领域，这称为分布式表示。%%HIGHLIGHT%% ==单词的分布式表示将单词表示为固定长度的向量。== %%POSTFIX%%这种向量的特征在于它是用密集向量表示的。密集向量的意思是，向量的*
>%%LINK%%[[#^ratkqqrhmnj|show annotation]]
>%%COMMENT%%
>单词的分布表示，就是处理成可以计算机处理的向量。
>%%TAGS%%
>#单词分布表示
^ratkqqrhmnj


>%%
>```annotation-json
>{"created":"2025-08-13T08:56:08.044Z","updated":"2025-08-13T08:56:08.044Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":66432,"end":66434},{"type":"TextQuoteSelector","exact":"(2","prefix":"明）： PMI(x,y)=log2P(x,y)P(x)P(y) ","suffix":".2)  第2章 自然语言和单词的分布式表示78其中，P(x)表"}]}]}
>```
>%%
>*%%PREFIX%%明）： PMI(x,y)=log2P(x,y)P(x)P(y)%%HIGHLIGHT%% ==(2== %%POSTFIX%%.2)  第2章 自然语言和单词的分布式表示78其中，P(x)表*
>%%LINK%%[[#^g0p35nd1isb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#点互信息（PMI), #PMI)
^g0p35nd1isb


>%%
>```annotation-json
>{"created":"2025-08-13T09:04:09.286Z","text":"奇异值分解的理解","updated":"2025-08-13T09:04:09.286Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":70479,"end":70499},{"type":"TextQuoteSelector","exact":"S是对角矩阵，奇异值在对角线上降序排列。","prefix":"了一些空间中的基轴（基向量），我们可以将矩阵U作为“单词空间”。","suffix":"简单地说，我们可以将奇异值视为“对应的基轴”的重要性。这样一来，"}]}]}
>```
>%%
>*%%PREFIX%%了一些空间中的基轴（基向量），我们可以将矩阵U作为“单词空间”。%%HIGHLIGHT%% ==S是对角矩阵，奇异值在对角线上降序排列。== %%POSTFIX%%简单地说，我们可以将奇异值视为“对应的基轴”的重要性。这样一来，*
>%%LINK%%[[#^mkzqp8ezprp|show annotation]]
>%%COMMENT%%
>奇异值分解的理解
>%%TAGS%%
>
^mkzqp8ezprp


>%%
>```annotation-json
>{"created":"2025-08-13T09:04:52.476Z","updated":"2025-08-13T09:04:52.476Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":70605,"end":70652},{"type":"TextQuoteSelector","exact":"矩阵S的奇异值小，对应的基轴的重要性低，因此，可以通过去除矩阵U中的多余的列向量来近似原始矩阵","prefix":"VT䃺ࢂID 䃺ࢂIDࢂ䃺ा䛼䭺㐡ऻ⮱ࢂ䃺ा䛼如图2-10所示，","suffix":"。用我们正在处理的“单词的PPMI矩阵”来说明的话，矩阵X的各行"}]}]}
>```
>%%
>*%%PREFIX%%VT䃺ࢂID 䃺ࢂIDࢂ䃺ा䛼䭺㐡ऻ⮱ࢂ䃺ा䛼如图2-10所示，%%HIGHLIGHT%% ==矩阵S的奇异值小，对应的基轴的重要性低，因此，可以通过去除矩阵U中的多余的列向量来近似原始矩阵== %%POSTFIX%%。用我们正在处理的“单词的PPMI矩阵”来说明的话，矩阵X的各行*
>%%LINK%%[[#^4xgr7s7ra1d|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^4xgr7s7ra1d


>%%
>```annotation-json
>{"created":"2025-08-13T09:01:27.847Z","text":"降维的方法：奇异值分解","updated":"2025-08-13T09:01:27.847Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":70189,"end":70234},{"type":"TextQuoteSelector","exact":"这里我们使用奇异值分解（Singular Value Decomposition，SVD）","prefix":"。这个密集矩阵就是我们想要的单词的分布式表示。降维的方法有很多，","suffix":"。 SVD将任意矩阵分解为3个矩阵的乘积，如下式所示： X=US"}]}]}
>```
>%%
>*%%PREFIX%%。这个密集矩阵就是我们想要的单词的分布式表示。降维的方法有很多，%%HIGHLIGHT%% ==这里我们使用奇异值分解（Singular Value Decomposition，SVD）== %%POSTFIX%%。 SVD将任意矩阵分解为3个矩阵的乘积，如下式所示： X=US*
>%%LINK%%[[#^z675mifksxg|show annotation]]
>%%COMMENT%%
>降维的方法：奇异值分解
>%%TAGS%%
>#奇异值分解, #Singular Value Decomposition
^z675mifksxg


>%%
>```annotation-json
>{"created":"2025-08-13T09:08:10.425Z","updated":"2025-08-13T09:08:10.425Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":72597,"end":72624},{"type":"TextQuoteSelector","exact":"Penn Treebank语料库（以下简称为PTB）。","prefix":"据作为语料库。这里，我们将使用一个大小合适的“真正的”语料库——","suffix":"PTB语料库经常被用作评价提案方法的基准。本书中我们将使用PTB"}]}]}
>```
>%%
>*%%PREFIX%%据作为语料库。这里，我们将使用一个大小合适的“真正的”语料库——%%HIGHLIGHT%% ==Penn Treebank语料库（以下简称为PTB）。== %%POSTFIX%%PTB语料库经常被用作评价提案方法的基准。本书中我们将使用PTB*
>%%LINK%%[[#^yc9hwi6vy2p|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^yc9hwi6vy2p


>%%
>```annotation-json
>{"created":"2025-08-13T09:08:15.027Z","updated":"2025-08-13T09:08:15.027Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":72666,"end":72720},{"type":"TextQuoteSelector","exact":"我们使用的PTB语料库在word2vec的发明者托马斯·米科洛夫（Tomas Mikolov）的网页上有提供","prefix":"作评价提案方法的基准。本书中我们将使用PTB语料库进行各种实验。","suffix":"。这个PTB语料库是以文本文件的形式提供的，与原始的PTB的文章"}]}]}
>```
>%%
>*%%PREFIX%%作评价提案方法的基准。本书中我们将使用PTB语料库进行各种实验。%%HIGHLIGHT%% ==我们使用的PTB语料库在word2vec的发明者托马斯·米科洛夫（Tomas Mikolov）的网页上有提供== %%POSTFIX%%。这个PTB语料库是以文本文件的形式提供的，与原始的PTB的文章*
>%%LINK%%[[#^vxdkimzfx|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^vxdkimzfx


>%%
>```annotation-json
>{"created":"2025-08-13T09:08:31.978Z","updated":"2025-08-13T09:08:31.978Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":72764,"end":72810},{"type":"TextQuoteSelector","exact":"括将稀有单词替换成特殊字符<unk>（unk是unknown的简称），将具体的数字替换成“N","prefix":"本文件的形式提供的，与原始的PTB的文章相比，多了若干预处理，包","suffix":"”等。下面，我们将经过这些预处理之后的文本数据作为PTB语料库使"}]}]}
>```
>%%
>*%%PREFIX%%本文件的形式提供的，与原始的PTB的文章相比，多了若干预处理，包%%HIGHLIGHT%% ==括将稀有单词替换成特殊字符<unk>（unk是unknown的简称），将具体的数字替换成“N== %%POSTFIX%%”等。下面，我们将经过这些预处理之后的文本数据作为PTB语料库使*
>%%LINK%%[[#^11xk7fl1vdmm|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^11xk7fl1vdmm


>%%
>```annotation-json
>{"created":"2025-08-13T08:59:43.452Z","updated":"2025-08-13T08:59:43.452Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":69764,"end":69792},{"type":"TextQuoteSelector","exact":"降维（dimensionality reduction）","prefix":"。对于这些问题，一个常见的方法是向量降维。2.4.2  降维所谓","suffix":"，顾名思义，就是减少向量维度。但是，并不是简单地减少，而是在尽量"}]}]}
>```
>%%
>*%%PREFIX%%。对于这些问题，一个常见的方法是向量降维。2.4.2  降维所谓%%HIGHLIGHT%% ==降维（dimensionality reduction）== %%POSTFIX%%，顾名思义，就是减少向量维度。但是，并不是简单地减少，而是在尽量*
>%%LINK%%[[#^vxz3a7ajgvf|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>#降维基础
^vxz3a7ajgvf


>%%
>```annotation-json
>{"created":"2025-08-13T13:09:12.791Z","updated":"2025-08-13T13:09:12.791Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":78628,"end":78688},{"type":"TextQuoteSelector","exact":"于推理的方法的主要操作是“推理”。如图3-2所示，当给出周围的单词（上下文）时，预测“？”处会出现什么单词，这就是推理。","prefix":"们会在3.5.3节再次讨论。3.1.2  基于推理的方法的概要基","suffix":"you and i say hello.goodbye?图3-2"}]}]}
>```
>%%
>*%%PREFIX%%们会在3.5.3节再次讨论。3.1.2  基于推理的方法的概要基%%HIGHLIGHT%% ==于推理的方法的主要操作是“推理”。如图3-2所示，当给出周围的单词（上下文）时，预测“？”处会出现什么单词，这就是推理。== %%POSTFIX%%you and i say hello.goodbye?图3-2*
>%%LINK%%[[#^ey1vpefpoo8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ey1vpefpoo8


>%%
>```annotation-json
>{"created":"2025-08-13T15:58:33.587Z","updated":"2025-08-13T15:58:33.587Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":79768,"end":79805},{"type":"TextQuoteSelector","exact":"只要将单词转化为固定长度的向量，神经网络的输入层的神经元个数就可以固定下来","prefix":"等的向量，并将单词ID对应的元素设为1，其他元素设为0。像这样，","suffix":"（图3-5）。youyou(1, 0, 0, 0, 0, 0, "}]}]}
>```
>%%
>*%%PREFIX%%等的向量，并将单词ID对应的元素设为1，其他元素设为0。像这样，%%HIGHLIGHT%% ==只要将单词转化为固定长度的向量，神经网络的输入层的神经元个数就可以固定下来== %%POSTFIX%%（图3-5）。youyou(1, 0, 0, 0, 0, 0,*
>%%LINK%%[[#^uvhu2eqlm4|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^uvhu2eqlm4


>%%
>```annotation-json
>{"created":"2025-08-13T16:01:01.896Z","updated":"2025-08-13T16:01:01.896Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":81679,"end":81708},{"type":"TextQuoteSelector","exact":"continuous bag-of-words（CBOW）","prefix":"-3所示的模型中。这里，我们使用由原版word2vec提出的名为","suffix":"的模型作为神经网络。word2vec一词最初用来指程序或者工具，"}]}]}
>```
>%%
>*%%PREFIX%%-3所示的模型中。这里，我们使用由原版word2vec提出的名为%%HIGHLIGHT%% ==continuous bag-of-words（CBOW）== %%POSTFIX%%的模型作为神经网络。word2vec一词最初用来指程序或者工具，*
>%%LINK%%[[#^5l4xd2wv8ys|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^5l4xd2wv8ys


>%%
>```annotation-json
>{"created":"2025-08-15T04:33:33.764Z","updated":"2025-08-15T04:33:33.764Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":81962,"end":81976},{"type":"TextQuoteSelector","exact":"CBOW模型的输入是上下文。","prefix":"型，使其能尽可能地进行正确的预测，我们可以获得单词的分布式表示。","suffix":"这个上下文用['you', 'goodbye'] 这样的单词列表"}]}]}
>```
>%%
>*%%PREFIX%%型，使其能尽可能地进行正确的预测，我们可以获得单词的分布式表示。%%HIGHLIGHT%% ==CBOW模型的输入是上下文。== %%POSTFIX%%这个上下文用['you', 'goodbye'] 这样的单词列表*
>%%LINK%%[[#^una2zbiesy|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^una2zbiesy


>%%
>```annotation-json
>{"created":"2025-08-15T04:42:21.031Z","text":"CBOW计算结束后使用的东西。","updated":"2025-08-15T04:42:21.031Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":82866,"end":82888},{"type":"TextQuoteSelector","exact":"权重Win的各行保存着各个单词的分布式表示。","prefix":"logoodbye(7×3)sayandi.W如图3-10所示，","suffix":"通过反复学习，不断更新各个单词的分布式表示，以正确地从上下文预测"}]}]}
>```
>%%
>*%%PREFIX%%logoodbye(7×3)sayandi.W如图3-10所示，%%HIGHLIGHT%% ==权重Win的各行保存着各个单词的分布式表示。== %%POSTFIX%%通过反复学习，不断更新各个单词的分布式表示，以正确地从上下文预测*
>%%LINK%%[[#^jkgqq38973a|show annotation]]
>%%COMMENT%%
>CBOW计算结束后使用的东西。
>%%TAGS%%
>
^jkgqq38973a


>%%
>```annotation-json
>{"created":"2025-08-15T16:44:46.975Z","updated":"2025-08-15T16:44:46.975Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":85137,"end":85147},{"type":"TextQuoteSelector","exact":"多类别分类的神经网络","prefix":"一下上述神经网络的学习。其实很简单，这里我们处理的模型是一个进行","suffix":"。因此，对其进行学习只是使用一下Softmax函数和交叉熵误差。"}]}]}
>```
>%%
>*%%PREFIX%%一下上述神经网络的学习。其实很简单，这里我们处理的模型是一个进行%%HIGHLIGHT%% ==多类别分类的神经网络== %%POSTFIX%%。因此，对其进行学习只是使用一下Softmax函数和交叉熵误差。*
>%%LINK%%[[#^ssyj6jqr5g|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ssyj6jqr5g


>%%
>```annotation-json
>{"created":"2025-08-15T16:44:50.658Z","updated":"2025-08-15T16:44:50.658Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":85163,"end":85178},{"type":"TextQuoteSelector","exact":"Softmax函数和交叉熵误差","prefix":"型是一个进行多类别分类的神经网络。因此，对其进行学习只是使用一下","suffix":"。首先，使用Softmax函数将得分转化为概率，再求这些概率和监"}]}]}
>```
>%%
>*%%PREFIX%%型是一个进行多类别分类的神经网络。因此，对其进行学习只是使用一下%%HIGHLIGHT%% ==Softmax函数和交叉熵误差== %%POSTFIX%%。首先，使用Softmax函数将得分转化为概率，再求这些概率和监*
>%%LINK%%[[#^lw5yqnv0mnh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^lw5yqnv0mnh


>%%
>```annotation-json
>{"created":"2025-08-19T05:37:27.738Z","text":"最受欢迎的分布式选择。特别是skip-gram模型。","updated":"2025-08-19T05:37:27.738Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":86203,"end":86212},{"type":"TextQuoteSelector","exact":"只使用输入侧的权重","prefix":"最终应该使用哪个权重作为单词的分布式表示呢？这里有三个选项。A.","suffix":"B.只使用输出侧的权重C.同时使用两个权重方案A和方案B只使用其"}]}]}
>```
>%%
>*%%PREFIX%%最终应该使用哪个权重作为单词的分布式表示呢？这里有三个选项。A.%%HIGHLIGHT%% ==只使用输入侧的权重== %%POSTFIX%%B.只使用输出侧的权重C.同时使用两个权重方案A和方案B只使用其*
>%%LINK%%[[#^3jr4cdlw8zl|show annotation]]
>%%COMMENT%%
>最受欢迎的分布式选择。特别是skip-gram模型。
>%%TAGS%%
>#分布式
^3jr4cdlw8zl
