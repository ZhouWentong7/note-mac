---
annotation-target: file:/Users/zhou-wentong/Documents/books/深度学习进阶-自然语言处理.pdf
---


>%%
>```annotation-json
>{"created":"2025-08-13T00:52:14.404Z","text":"Githu仓库\n","updated":"2025-08-13T00:52:14.404Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":17756,"end":17817},{"type":"TextQuoteSelector","exact":"https://github.com/oreilly-japan/deep-learning-from-scratch-2","prefix":"ly.co.jp/books/9784873118369（日语）","suffix":"  前言xx关于O’Reilly的其他信息，可以访问下面的O’R"}]}]}
>```
>%%
>*%%PREFIX%%ly.co.jp/books/9784873118369（日语）%%HIGHLIGHT%% ==https://github.com/oreilly-japan/deep-learning-from-scratch-2== %%POSTFIX%%前言xx关于O’Reilly的其他信息，可以访问下面的O’R*
>%%LINK%%[[#^fposwz6dju9|show annotation]]
>%%COMMENT%%
>Githu仓库
>
>%%TAGS%%
>#github, #code
^fposwz6dju9


>%%
>```annotation-json
>{"created":"2025-08-13T01:08:51.425Z","text":"多类别分类：有很多类别，但只属于其中某一类\n多标签：有很多类别，可能被分配多个标签（使用sigmoid）","updated":"2025-08-13T01:08:51.425Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":28019,"end":28068},{"type":"TextQuoteSelector","exact":"进行多类别分类的神经网络通常使用交叉熵误差（cross entropy error）作为损失函数。","prefix":"计算神经网络的损失要使用损失函数（loss function）。","suffix":"此时，交叉熵误差由神经网络输出的各类别的概率和监督标签求得。现在"}]}]}
>```
>%%
>*%%PREFIX%%计算神经网络的损失要使用损失函数（loss function）。%%HIGHLIGHT%% ==进行多类别分类的神经网络通常使用交叉熵误差（cross entropy error）作为损失函数。== %%POSTFIX%%此时，交叉熵误差由神经网络输出的各类别的概率和监督标签求得。现在*
>%%LINK%%[[#^ud1pfn0cuxq|show annotation]]
>%%COMMENT%%
>多类别分类：有很多类别，但只属于其中某一类
>多标签：有很多类别，可能被分配多个标签（使用sigmoid）
>%%TAGS%%
>#ce-loss
^ud1pfn0cuxq


>%%
>```annotation-json
>{"created":"2025-08-13T01:13:20.344Z","text":"交叉熵误差","updated":"2025-08-13T01:13:20.344Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":28743,"end":28760},{"type":"TextQuoteSelector","exact":"=−∑ktklogyk (1.7)","prefix":"，这个概率会被输入交叉熵误差。此时，交叉熵误差可由下式表示： L","suffix":"  第1章 神经网络的复习20这里，tk是对应于第k个类别的监督"}]}]}
>```
>%%
>*%%PREFIX%%，这个概率会被输入交叉熵误差。此时，交叉熵误差可由下式表示： L%%HIGHLIGHT%% ===−∑ktklogyk (1.7)== %%POSTFIX%%第1章 神经网络的复习20这里，tk是对应于第k个类别的监督*
>%%LINK%%[[#^xf713toupbi|show annotation]]
>%%COMMENT%%
>交叉熵误差
>%%TAGS%%
>
^xf713toupbi


>%%
>```annotation-json
>{"created":"2025-08-13T01:18:39.894Z","text":"有多个自变量时，分别计算因变量关于每个自变量的偏导，罗列到一起，就是梯度（gradient）。\n严格来说，和数学领域中的梯度略有区别。","updated":"2025-08-13T01:18:39.894Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":29964,"end":29987},{"type":"TextQuoteSelector","exact":"将关于向量各个元素的导数罗列到一起，就得到了梯","prefix":"∂L∂x1,∂L∂x2,...,∂L∂xn) (1.9)像这样，","suffix":"度（gradient）。另外，矩阵也可以像向量一样求梯度。假设W"}]}]}
>```
>%%
>*%%PREFIX%%∂L∂x1,∂L∂x2,...,∂L∂xn) (1.9)像这样，%%HIGHLIGHT%% ==将关于向量各个元素的导数罗列到一起，就得到了梯== %%POSTFIX%%度（gradient）。另外，矩阵也可以像向量一样求梯度。假设W*
>%%LINK%%[[#^gmqohl5bqpq|show annotation]]
>%%COMMENT%%
>有多个自变量时，分别计算因变量关于每个自变量的偏导，罗列到一起，就是梯度（gradient）。
>严格来说，和数学领域中的梯度略有区别。
>%%TAGS%%
>#梯度, #gradient
^gmqohl5bqpq


>%%
>```annotation-json
>{"created":"2025-08-13T01:18:13.974Z","text":"导数含义","updated":"2025-08-13T01:18:13.974Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":29646,"end":29683},{"type":"TextQuoteSelector","exact":"就是x的微小（严格地讲，“微小”为无限小）变化会导致y发生多大程度的变化。","prefix":"的导数记为 dydx。这个 dydx的意思是变化程度，具体来说，","suffix":"比如函数y=x2，其导数可以解析性地求出，即 dydx=2x。这"}]}]}
>```
>%%
>*%%PREFIX%%的导数记为 dydx。这个 dydx的意思是变化程度，具体来说，%%HIGHLIGHT%% ==就是x的微小（严格地讲，“微小”为无限小）变化会导致y发生多大程度的变化。== %%POSTFIX%%比如函数y=x2，其导数可以解析性地求出，即 dydx=2x。这*
>%%LINK%%[[#^4dxioxzs5we|show annotation]]
>%%COMMENT%%
>导数含义
>%%TAGS%%
>
^4dxioxzs5we


>%%
>```annotation-json
>{"created":"2025-08-13T01:20:10.692Z","text":"和数学中的梯度的区别\n","updated":"2025-08-13T01:20:10.692Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":30330,"end":30379},{"type":"TextQuoteSelector","exact":"数学中的梯度仅限于关于向量的导数。而在深度学习领域，一般也会定义关于矩阵和张量的导数，称为“梯度”","prefix":"现。严格地说，本书使用的“梯度”一词与数学中的“梯度”是不同的。","suffix":"。1.3.3  链式法则学习阶段的神经网络在给定学习数据后会输出"}]}]}
>```
>%%
>*%%PREFIX%%现。严格地说，本书使用的“梯度”一词与数学中的“梯度”是不同的。%%HIGHLIGHT%% ==数学中的梯度仅限于关于向量的导数。而在深度学习领域，一般也会定义关于矩阵和张量的导数，称为“梯度”== %%POSTFIX%%。1.3.3  链式法则学习阶段的神经网络在给定学习数据后会输出*
>%%LINK%%[[#^cot1wtvxc65|show annotation]]
>%%COMMENT%%
>和数学中的梯度的区别
>
>%%TAGS%%
>
^cot1wtvxc65


>%%
>```annotation-json
>{"created":"2025-08-13T07:40:49.862Z","text":"找到损失增加最多的方向，顺着这个方向反向更新就下降最快\n","updated":"2025-08-13T07:40:49.862Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":39329,"end":39383},{"type":"TextQuoteSelector","exact":"这个梯度指向当前的权重参数所处位置中损失增加最多的方向。因此，通过将参数向该梯度的反方向更新，可以降低损失。","prefix":"选择mini-batch数据，根据误差反向传播法获得权重的梯度。","suffix":"这就是梯度下降法（gradient descent）。之后，根据"}]}]}
>```
>%%
>*%%PREFIX%%选择mini-batch数据，根据误差反向传播法获得权重的梯度。%%HIGHLIGHT%% ==这个梯度指向当前的权重参数所处位置中损失增加最多的方向。因此，通过将参数向该梯度的反方向更新，可以降低损失。== %%POSTFIX%%这就是梯度下降法（gradient descent）。之后，根据*
>%%LINK%%[[#^8fqldro6ye|show annotation]]
>%%COMMENT%%
>找到损失增加最多的方向，顺着这个方向反向更新就下降最快
>
>%%TAGS%%
>
^8fqldro6ye


>%%
>```annotation-json
>{"created":"2025-08-13T01:12:32.484Z","text":"所有类别的输出和为1，可以解释为概率。\n","updated":"2025-08-13T01:12:32.484Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":28639,"end":28666},{"type":"TextQuoteSelector","exact":"Softmax函数输出的各个元素是0.0～1.0的实数","prefix":"数的分子是得分sk的指数函数，分母是所有输入信号的指数函数的和。","suffix":"。另外，如果将这些元素全部加起来，则和为1。因此，Softmax"}]}]}
>```
>%%
>*%%PREFIX%%数的分子是得分sk的指数函数，分母是所有输入信号的指数函数的和。%%HIGHLIGHT%% ==Softmax函数输出的各个元素是0.0～1.0的实数== %%POSTFIX%%。另外，如果将这些元素全部加起来，则和为1。因此，Softmax*
>%%LINK%%[[#^2kenz90y9vo|show annotation]]
>%%COMMENT%%
>所有类别的输出和为1，可以解释为概率。
>
>%%TAGS%%
>
^2kenz90y9vo
