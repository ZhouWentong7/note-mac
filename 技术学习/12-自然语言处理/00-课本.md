---
annotation-target: file:/Users/zhou-wentong/Documents/books/深度学习进阶-自然语言处理.pdf
---


>%%
>```annotation-json
>{"created":"2025-08-13T00:52:14.404Z","text":"Githu仓库\n","updated":"2025-08-13T00:52:14.404Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":17756,"end":17817},{"type":"TextQuoteSelector","exact":"https://github.com/oreilly-japan/deep-learning-from-scratch-2","prefix":"ly.co.jp/books/9784873118369（日语）","suffix":"  前言xx关于O’Reilly的其他信息，可以访问下面的O’R"}]}]}
>```
>%%
>*%%PREFIX%%ly.co.jp/books/9784873118369（日语）%%HIGHLIGHT%% ==https://github.com/oreilly-japan/deep-learning-from-scratch-2== %%POSTFIX%%前言xx关于O’Reilly的其他信息，可以访问下面的O’R*
>%%LINK%%[[#^fposwz6dju9|show annotation]]
>%%COMMENT%%
>Githu仓库
>
>%%TAGS%%
>#github, #code
^fposwz6dju9


>%%
>```annotation-json
>{"created":"2025-08-13T01:08:51.425Z","text":"多类别分类：有很多类别，但只属于其中某一类\n多标签：有很多类别，可能被分配多个标签（使用sigmoid）","updated":"2025-08-13T01:08:51.425Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":28019,"end":28068},{"type":"TextQuoteSelector","exact":"进行多类别分类的神经网络通常使用交叉熵误差（cross entropy error）作为损失函数。","prefix":"计算神经网络的损失要使用损失函数（loss function）。","suffix":"此时，交叉熵误差由神经网络输出的各类别的概率和监督标签求得。现在"}]}]}
>```
>%%
>*%%PREFIX%%计算神经网络的损失要使用损失函数（loss function）。%%HIGHLIGHT%% ==进行多类别分类的神经网络通常使用交叉熵误差（cross entropy error）作为损失函数。== %%POSTFIX%%此时，交叉熵误差由神经网络输出的各类别的概率和监督标签求得。现在*
>%%LINK%%[[#^ud1pfn0cuxq|show annotation]]
>%%COMMENT%%
>多类别分类：有很多类别，但只属于其中某一类
>多标签：有很多类别，可能被分配多个标签（使用sigmoid）
>%%TAGS%%
>#ce-loss
^ud1pfn0cuxq


>%%
>```annotation-json
>{"created":"2025-08-13T01:13:20.344Z","text":"交叉熵误差","updated":"2025-08-13T01:13:20.344Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":28743,"end":28760},{"type":"TextQuoteSelector","exact":"=−∑ktklogyk (1.7)","prefix":"，这个概率会被输入交叉熵误差。此时，交叉熵误差可由下式表示： L","suffix":"  第1章 神经网络的复习20这里，tk是对应于第k个类别的监督"}]}]}
>```
>%%
>*%%PREFIX%%，这个概率会被输入交叉熵误差。此时，交叉熵误差可由下式表示： L%%HIGHLIGHT%% ===−∑ktklogyk (1.7)== %%POSTFIX%%第1章 神经网络的复习20这里，tk是对应于第k个类别的监督*
>%%LINK%%[[#^xf713toupbi|show annotation]]
>%%COMMENT%%
>交叉熵误差
>%%TAGS%%
>
^xf713toupbi


>%%
>```annotation-json
>{"created":"2025-08-13T01:18:39.894Z","text":"有多个自变量时，分别计算因变量关于每个自变量的偏导，罗列到一起，就是梯度（gradient）。\n严格来说，和数学领域中的梯度略有区别。","updated":"2025-08-13T01:18:39.894Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":29964,"end":29987},{"type":"TextQuoteSelector","exact":"将关于向量各个元素的导数罗列到一起，就得到了梯","prefix":"∂L∂x1,∂L∂x2,...,∂L∂xn) (1.9)像这样，","suffix":"度（gradient）。另外，矩阵也可以像向量一样求梯度。假设W"}]}]}
>```
>%%
>*%%PREFIX%%∂L∂x1,∂L∂x2,...,∂L∂xn) (1.9)像这样，%%HIGHLIGHT%% ==将关于向量各个元素的导数罗列到一起，就得到了梯== %%POSTFIX%%度（gradient）。另外，矩阵也可以像向量一样求梯度。假设W*
>%%LINK%%[[#^gmqohl5bqpq|show annotation]]
>%%COMMENT%%
>有多个自变量时，分别计算因变量关于每个自变量的偏导，罗列到一起，就是梯度（gradient）。
>严格来说，和数学领域中的梯度略有区别。
>%%TAGS%%
>#梯度, #gradient
^gmqohl5bqpq


>%%
>```annotation-json
>{"created":"2025-08-13T01:18:13.974Z","text":"导数含义","updated":"2025-08-13T01:18:13.974Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":29646,"end":29683},{"type":"TextQuoteSelector","exact":"就是x的微小（严格地讲，“微小”为无限小）变化会导致y发生多大程度的变化。","prefix":"的导数记为 dydx。这个 dydx的意思是变化程度，具体来说，","suffix":"比如函数y=x2，其导数可以解析性地求出，即 dydx=2x。这"}]}]}
>```
>%%
>*%%PREFIX%%的导数记为 dydx。这个 dydx的意思是变化程度，具体来说，%%HIGHLIGHT%% ==就是x的微小（严格地讲，“微小”为无限小）变化会导致y发生多大程度的变化。== %%POSTFIX%%比如函数y=x2，其导数可以解析性地求出，即 dydx=2x。这*
>%%LINK%%[[#^4dxioxzs5we|show annotation]]
>%%COMMENT%%
>导数含义
>%%TAGS%%
>
^4dxioxzs5we


>%%
>```annotation-json
>{"created":"2025-08-13T01:20:10.692Z","text":"和数学中的梯度的区别\n","updated":"2025-08-13T01:20:10.692Z","document":{"title":"","link":[{"href":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451"}],"documentFingerprint":"d4305cfa0dbbab47b5493eec12ef5451"},"uri":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","target":[{"source":"urn:x-pdf:d4305cfa0dbbab47b5493eec12ef5451","selector":[{"type":"TextPositionSelector","start":30330,"end":30379},{"type":"TextQuoteSelector","exact":"数学中的梯度仅限于关于向量的导数。而在深度学习领域，一般也会定义关于矩阵和张量的导数，称为“梯度”","prefix":"现。严格地说，本书使用的“梯度”一词与数学中的“梯度”是不同的。","suffix":"。1.3.3  链式法则学习阶段的神经网络在给定学习数据后会输出"}]}]}
>```
>%%
>*%%PREFIX%%现。严格地说，本书使用的“梯度”一词与数学中的“梯度”是不同的。%%HIGHLIGHT%% ==数学中的梯度仅限于关于向量的导数。而在深度学习领域，一般也会定义关于矩阵和张量的导数，称为“梯度”== %%POSTFIX%%。1.3.3  链式法则学习阶段的神经网络在给定学习数据后会输出*
>%%LINK%%[[#^cot1wtvxc65|show annotation]]
>%%COMMENT%%
>和数学中的梯度的区别
>
>%%TAGS%%
>
^cot1wtvxc65
