前馈型网络的局限性：不能很好的处理是序列问题。

RNN（Recurrent Neural Network，循环神经网络）由此诞生。
#RNN

## 概率和语言模型  

1. CBOW模型的目的：从上下文预测目标词
2. 若将上下文限定为左侧窗口，则概率公式：$P(w_t|w_{t-2},w_{t-1})$

### 语言模型（language model）
给出单词序列发生的概率——用一个序列评估一个单词序列发生的可能性。

假设m个单词顺序出现为$P(w_1,...,w_m)$，这个是多个事件一起发生，是联合概率的一种情况。

则可分解为：
$$
P(w_1,...,w_m) = P(w_m|w_1,...,w_{m-1})P(w_{m-1}|w_1,...,w_{m-2})...P(w_2|w_1)P(w_1)=\Pi_{t=1}^mP(w_t,...,w_{t-1})
$$
> 联合概率可以由后验概率的乘积表示。

> 补充：
> $P(A,B) = P(A|B)P(A)$
> A 和B共同发生的概率等于 B发生的概率和 B发生后A发生的概率的乘积。

上述公式的后验概率乘积，是以目标词左侧的全部单词为上下文（条件）时的概率。

目标：

求得$P(w_t|w_1,...,w_{t-1})$的概率。
—— 这种模型为条件语言模型（conditional language model).
求出后，就能求的语言模型的联合概率$P(w_1,...,w_m)$.


### 5.1.3 CBOW 模型用作语言模型？
只能用近似的方法，但上下文窗口太小则会失去准度，太大，CBOW模型还存在忽视了上下文单词顺序的问题。

> #马尔科夫模型
> 未来状态只依存于当前状态。
> 当某个事件的概率仅取决于前N个状态，称为“N阶马尔科夫链”。

RNN：可以处理任意长度的时序数据。但是
- 长距离梯度消失：大规模数据，单词之间的语义关系难以捕捉
word2vec优势：
- 小、快、好用

## 5.2 RNN


